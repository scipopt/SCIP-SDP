\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[lined,ruled,noend,linesnumbered]{algorithm2e}
\usepackage[color=red!50!blue!50,textsize=footnotesize,textwidth=4cm]{todonotes}
\usepackage[breaklinks,colorlinks,citecolor=blue,linkcolor=blue]{hyperref}
% \usepackage[tmargin=2.5cm,bmargin=2.5cm,lmargin=2.8cm,rmargin=2.8cm]{geometry}
\usepackage{cleveref}
\usepackage{xspace}

% operators
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\relint}{relint}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\renewcommand{\Re}{\text{Re}}

% macros
\newcommand{\suchthat}{\,:\,}
\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\card}[1]{\lvert{#1}\rvert}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\define}{\coloneqq}
\newcommand{\enifed}{\eqqcolon}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\Norm}[2]{\lVert{#1}\rVert_{#2}}
\newcommand{\skal}[2]{\langle{#1},{#2}\rangle}
\newcommand{\order}[1]{O\left({#1}\right)}
\newcommand{\T}{^{\top}}

% sets
\newcommand{\R}{\mathds{R}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\C}{\mathds{C}}
\newcommand{\E}{\mathds{E}}
\newcommand{\ones}{\mathds{1}}
\renewcommand{\P}{\mathds{P}}
\renewcommand{\E}{\mathds{E}}

% environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}

% adjust itemize
\setlist[itemize]{topsep=0.5ex,partopsep=0ex,parsep=0ex,itemsep=0.5ex}

% other defines
\newcommand{\MP}[1]{\todo{#1}}
\newcommand{\MPin}[1]{\todo[inline]{#1}}


% title
\title{Presolving for\\ Mixed-Integer Semidefinite Optimization}
\author{Marc E. Pfetsch\thanks{Department of Mathematics, TU Darmstadt, Germany}}
\date{September 2020}

% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\begin{document}

\maketitle

\todo[inline]{FM: We should argue that alle the inequalities and bounds are
  as strong as possible.}

\section{Introduction}

We consider mixed-integer semidefinite programs (MISDP) of the following
form:
\begin{equation}\label{MISDP}
  \begin{aligned}
    \inf \quad & b\T y \\
    \text{s.t.} \quad & \sum_{i=1}^m A^i\, y_i - A^0 \succeq 0, \\
    & \ell_i \leq y_i \leq u_i && \forall\, i \in [m], \\
    & y_i \in \Z && \forall\, i \in I,
  \end{aligned}
\end{equation}
with symmetric matrices $A^i \in \R^{n \times n}$ for
$i \in [m]_0 \define \{0, \dots, m\}$, $b \in \R^m$,
$\ell_i \in \R \cup \{- \infty\}$, $u_i \in \R \cup \{\infty\}$ for all
$i \in [m] \define \{1, \dots, m\}$. The set of indices of integer
variables is given by $I \subseteq [m]$.

We also consider the following form -- often called ``primal'' form:
\begin{equation}\label{MISDP-P}
  \begin{aligned}
    \inf \quad & \skal{A^0}{X} \\
    \text{s.t.} \quad & \skal{A^i}{X} = b_i && \forall\, i \in [m] \\
    & L_{ij} \leq X_{ij} \leq U_{ij} && \forall\, i,\,j \in [n], \\
    & X_{ij} \in \Z && \forall\, i,\,j \in I \times I,\\
    & X \succeq 0.
  \end{aligned}
\end{equation}
Here the bounds are given by $L_{ij} \in \R \cup \{- \infty\}$, $U_{ij} \in \R \cup \{\infty\}$ for all
$i$, $j \in [m]$.

In this manuscript we try to derive presolving methods for the above two
problems. For instance we try to generalize the presolving techniques from
the MIP to the MISDP case. For instance, we can consider the techniques
demonstrated by Savelsbergh~\cite{Sav94}.

\paragraph{Notation}

We use the notation $A(y) \define \sum_{i=1}^m A^i\, y_i - A^0$ for $y \in \R^m$.


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\section{Possibly Useful Procedures}

One basic argument that we will use repeatedly is the following. Consider a
positive semidefinite (psd) matrix $A \in \R^{n \times n}$. Then the
diagonal entries are nonnegative (since $0 \leq e_i\T A e_i =
A_{ii}$). Then taking the $2 \times 2$-minor for $i$ and $j \in [n]$, we
get $A_{ii}\, A_{jj} - A_{ij}^2 \geq 0$. This implies
\begin{equation}\label{eq:TwoByTwoMinorInequality}
  \abs{A_{ij}} \leq \sqrt{A_{ii} A_{jj}} \leq \frac{A_{ii} + A_{jj}}{2} \leq \max\,\{A_{ii},
  A_{jj}\},
\end{equation}
where we used inequality of geometric and arithmetic means for the second inequality.

This has the following two implications (see, e.g., Observation 1.1.5 and
1.1.6 in Helmberg~\cite{Hel00}):
\begin{enumerate}
\item There the always exists a diagonal entry of maximal absolute value,
  i.e., there exists $r \in [n]$ with
  \[
    A_{rr} = \max\, \{\abs{A_{ij}} \suchthat i,\,j \in [n]\}.
  \]
\item If $A_{ii} = 0$ then $A_{ij} = 0$ for all $j \in [n]$.
\end{enumerate}

\begin{question}
  Is the second condition enforced somehow -- in the sense that the bounds
  are fixed? For example if we branch variables in~\eqref{MISDP} such that
  a diagonal entry of~$A(y)$ becomes 0, are the other entries in the column and row
  fixed to 0 as well?
\end{question}


% -------------------------------------------------------------------------
\subsection{Implied Linear Inequalities}\label{sec:ImpliedLinEqs}

The following inequalities are known from the literature, see
Mars~\cite{Mar13} and  Gally~\cite{Gal19}, and can be added as linear inequalities w.r.t.\
\eqref{MISDP}:
\begin{itemize}
\item Any feasible $y \in \R^m$ satisfies $A(y) \succeq 0$, which implies
  that the diagonal entries of $A(y)$ are nonnegative, i.e., for all
  $i \in [n]$:
  \begin{align}\label{eq:diaggezero}
    \sum_{k = 1}^m (A^k)_{ii}\, y_k - (A^0)_{ii} \geq 0.
  \end{align}
\item If for some $i$, $j \in [n]$, $A^0_{ij} \neq 0$, and $A^0_{ii} = 0$,
  $A^k_{ij} = 0$ for all $k \in [m]$, $A^k_{ii} = 0$ for all continuous
  variables and $\ell_i \geq 0$ for all integer variables,
  the following inequality is valid:
  \begin{align}\label{eq:diagzeroimpl}
    \sum_{\substack{k \in \mathcal{I}:\\ A^k_{ii} > 0}} y_k \geq1.
  \end{align}
  Note that the conditions imply that $A^0$ is not psd.
\item For all $X \succeq 0$, we have
  \begin{align}\label{eq:2minorlinP}
    X_{ii} + X_{jj} - 2\, X_{ij} \geq 0,\qquad
    X_{ii} + X_{jj} + 2\, X_{ij} \geq 0.
  \end{align}
  This follows by using a 2 by 2 minor and multiplying from left
  and right by the all-ones vector and $[1,-1]$, respectively. It also
  follows from~\eqref{eq:TwoByTwoMinorInequality}.
  Note that an additional rank-1 constraint on the matrix~$X$ does not lead
  to stronger inequalities.
\item The previous inequalities translated to the matrix pencil are:
  \begin{equation}\label{eq:2minorlinD}
    \begin{aligned}
      & \sum_{k=1}^m A^k_{ii}\, y_k - A^0_{ii} + \sum_{k=1}^m A^k_{jj}\, y_k - A^0_{jj} - 2 \Big(\sum_{k=1}^m A^k_{ij}\, y_k - A^0_{ij}\Big) \geq 0\\
      \quad\Leftrightarrow\quad& \sum_{k=1}^m \Big(A ^k_{ii} + A^k_{jj} -
      2\, A^k_{ij}\Big)\, y_k \geq A^0_{ii} + A^0_{jj} - 2 A^0_{ij}.
    \end{aligned}
  \end{equation}
\item If $A^k_{ii} = A^k_{jj} = 0$ for all~$k \in [m]$
  and~$A^0_{ii}A^0_{jj} \geq 0$ for some $i \neq j \in [n]$, then the
  following inequality is valid:
  \begin{align}\label{eq:2minorprodD}
    \sum_{k=1}^m A^k_{ij} y_k \geq A^0 - \sqrt{A^0_{ii}A^0_{jj}}.
  \end{align}
  Furhtermore, if exactly one~$A^k_{ij} \neq 0$, then this yields upper or
  lower bounds for the corresponding variable~$y_k$, depending on the sign
  of~$A^k_{ij}$.
\item For a positive semidefinite matrix $X$ the following quadratic
  inequality holds by using the 2 by 2 minor:
  $X_{st}^2 \leq X_{ss} X_{tt}$. This constraint is SOC representable:
  \begin{align*}
    \left\|\binom{2\,X_{st}}{X_{ss} - X_{tt}}\right\| &\leq X_{ss} +
                                                        X_{tt}. \\
    \quad\Leftrightarrow\quad 4\, X_{st}^2 + (X_{ss}^2 - 2\,X_{ss}\, X_{tt}
    + X_{tt}^2) & \leq X_{ss}^2 + 2\, X_{ss}\,X_{tt} + X_{tt}^2.
  \end{align*}
  Translated to the matrix pencil notation the cut looks as follows:
  \begin{equation}\label{eq:2minorSOC}
    \begin{aligned}
      &\left\|\binom{2\, \sum_{i=1}^m (A_i)_{st}\, y_i - 2\, (A_0)_{st}}{ q
          \sum_{i=1}^m (A_i)_{ss}\, y_i - (A_0)_{ss} - \sum_{i=1}^m
          (A_i)_{tt}\, y_i + (A_0)_{tt}}\right\| \\ &\leq \sum_{i=1}^m
      (A_i)_{ss}\, y_i - (A_0)_{ss} + \sum_{i=1}^m (A_i)_{tt}\, y_i -
      (A_0)_{tt}.
    \end{aligned}
  \end{equation}
  These constraints can be added directly as SOC-constraints.
\end{itemize}

All the above methods are implemented and seem to help for some
instances.

\begin{remark}
  The assumptions under which inequalities~\eqref{eq:diagzeroimpl} are
  applicable can be weakened. These inequalities are also valid
  if~$A^0_{ii} > 0$, since, under the remaining assumptions,
  \begin{align*}
    0 \leq A(y)_{ii} \cdot A(y)_{jj} - (A^0_{ij})^2 \quad &\implies \quad
    0 < A(y)_{ii} = \sum_{k} A^k_{ii}y_k - A^0_{ii}, \\
    &\implies \quad 0 < \sum_{k} A^k_{ii}y_k
  \end{align*}
  and thus~$0 < \sum_{k} A^k_{ii}y_k$ has to hold. This implies the
  validity of~\eqref{eq:diagzeroimpl}.
\end{remark}

% -------------------------------------------------------------------------
\subsection{Presolving for $\boldsymbol{2 \times 2}$-minors}
\label{sec:2by2Minors}

\begin{lemma}
  Consider a solution $X$ to~\eqref{MISDP-P}. Then
  \[
    -\sqrt{U_{ii}\, U_{jj}} \leq X_{ij} \leq \sqrt{U_{ii}\, U_{jj}}
  \]
  holds for all $i$, $j \in [n]$.
\end{lemma}

\begin{proof}
  Since $X$ is positive semidefinite, we have
  \[
    X_{ii} X_{jj} - X_{ij}^2 \geq 0
    \quad\Leftrightarrow\quad
    X_{ij}^2 \leq X_{ii} X_{jj}.
  \]
  Moreover, since the diagonal terms of $X$ are nonnegative, we have
  $U_{ii} \geq L_{ii} \geq 0$ for all $i \in [n]$. Thus,

  Upper bounding via
  McCormick inequalities yields:
  \begin{align*}
    X_{ii} X_{jj} & \leq \min \{L_{jj}\, X_{ii} + U_{ii}\, X_{jj} - U_{ii}\, L_{jj},\;
                    U_{jj}\, X_{ii} + L_{ii}\, X_{jj} - L_{ii}\, U_{jj}\}  \\
    & \leq \min \{L_{jj}\, U_{ii} + U_{ii}\, U_{jj} - U_{ii}\, L_{jj},\;
                    U_{jj}\, U_{ii} + L_{ii}\, U_{jj} - L_{ii}\, U_{jj}\}  \\
    & = \min \{U_{ii}\, U_{jj},\;  U_{jj}\, U_{ii}\} = U_{ii}\, U_{jj}.
  \end{align*}
  Taking the square root gives the claim.
\end{proof}
Note that these bounds are tight, even for a rank-1 matrix~$X$: Consider
the all-ones matrix, which clearly has rank~1.

This result can partly be translated to the matrix pencil format by defining
\[
  \tilde{U}_{ij} \define \sum_{k \in [m]: A^k_{ij} > 0} A^k_{ij}\, u_k -
  \sum_{k \in [m]: A^k_{ij} < 0} A^k_{ij}\, \ell_k - A^0_{ij}.
\]
Thus for any $y \in [\ell,u]$ we have $A(y)_{ij} \leq \tilde{U}_{ij}$. This
directly yields:

\begin{lemma}\label{lem:Propagation}
  For any solution $y \in \R^m$ of~\eqref{MISDP}, we have
  \[
    -\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} \leq A(y)_{ij} \leq \sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}}
  \]
  for all $i$, $j \in [n]$.
\end{lemma}

Currently this method is implemented for the case in which each entry
$(i,j)$ is contained in exactly one matrix, i.e., there exists
$k = k(i,j) \in [m]$ such that
\[
  A(y)_{ij} = A^k_{ij}\, y_k - A^0_{ij}.
\]
In this case, we have
\[
  \frac{-\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} + A^0_{ij}}{A^k_{ij}} \leq y_k \leq
  \frac{\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} + A^0_{ij}}{A^k_{ij}}.
\]
Unfortunately, this has no effect on our test instances, i.e., the bounds
could never be strengthened across the large testset.

\begin{remark}
  Note that in the pure form, the propagated inequalities from
  Lemma~\ref{lem:Propagation} dominate the inequalities
  in~\eqref{eq:2minorlinP}:
  \begin{align*}
    X_{st} \leq \sqrt{U_{ss} \cdot U_{tt}} \leq \frac{1}{2}\big( U_{ss} + U_{tt}\big),
  \end{align*}
  because of the inequality between the arithmetic and geometric mean. Does
  this also hold in the general case of a (dual) LMI?
\end{remark}

\begin{question}
  Can this be effective on some instance? Should this be applied
  nevertheless? Does it make sense to extend the method for the general
  case in~\eqref{MISDP}?
\end{question}

% -------------------------------------------------------------------------
\subsection{Implications of Trace Constraints}

The following is an extension of an observation in the context of computing
the RIP, see~\cite{GalP16}.

\begin{lemma}
  Consider~\eqref{MISDP-P} in which $\tr(X) \leq \alpha$ has to hold. Then
  \[
    -\tfrac{\alpha}{2} \leq X_{ij} \leq \tfrac{\alpha}{2}
  \]
  holds for all $i$, $j \in [n]$.
\end{lemma}

\begin{proof}
  Since $X$ is positive semidefinite, we have
  \[
    X_{ii} X_{jj} - X_{ij}^2 \geq 0
    \quad\Leftrightarrow\quad
    X_{ij}^2 \leq X_{ii} X_{jj}.
  \]
  We observe that $X_{ii} + X_{jj} \leq \alpha$ and $X_{ii}$,
  $X_{jj} \geq 0$. Thus
  \[
    X_{ii} X_{jj} \leq X_{ii} (\alpha - X_{ii}) = \alpha X_{ii} - X_{ii}^2.
  \]
  Taking the derivative and equating 0 yields a maximal point
  $X_{ii}^\star = \tfrac{\alpha}{2}$. Consequently,
  \[
    X_{ij}^2 \leq X_{ii} X_{jj} \leq \alpha X_{ii}^\star - (X_{ii}^\star)^2 =
    \tfrac{\alpha^2}{2} - \tfrac{\alpha^2}{4} = \tfrac{\alpha^2}{4}.
  \]
  Taking the square root shows the claim.
\end{proof}

\begin{question}
  Should this be transferred to~\eqref{MISDP} and how?
\end{question}

This is also implemented if each matrix entry arises from a single matrix
$A^k$ and $A^0$ is 0 for all involved entries. It does not find a
strengthening for the testset, but it strengthens the bounds for RIP
instances if the bounds have not been tightened before.

% -------------------------------------------------------------------------
\subsection{``Coefficient Tightening''}
\label{sec:CoefficientTightening}

For a linear constraint, coefficient tightening can be explained by the
following example: Consider two binary variables~$x$ and $y$ and the
constraint $x + 2\, y \geq 1$. Then the coefficient of $y$ can be tightened
to $1$, yielding $x + y \geq 1$.

Assume that all matrices $A^k$, $k \in [m]$, are psd and $y_k$ is binary
for $k \in [m]$. Then we can compute
\begin{equation}\label{eq:ComputeHatMu}
  \hat{\mu}_k = \min\, \{ \mu \geq 0 \suchthat \mu A^k - A^0 \succeq 0\}
\end{equation}
and define $\mu_k \define \min\, \{\hat{\mu}_k, 1\}$. Note that $\mu_k = 1$ if
$\hat{\mu}_k = \infty$, i.e., if the above optimization problem is infeasible.

\begin{lemma}\label{lem:Tightening1}
  Let $k \in [m]$ with $A^k \succeq 0$. Then
  \[
    A^k - A^0 \succeq 0
    \quad\Leftrightarrow\quad
    \mu_k\, A^k - A^0 \succeq 0.
  \]
\end{lemma}

\begin{proof}
  Assume that $A^k - A^0 \succeq 0$. This implies that $\hat{\mu}_k \leq 1$
  and thus $\mu_k = \hat{\mu}_k$. This implies that
  $\mu_k\, A^k - A^0 \succeq 0$ by definition of $\hat{\mu}_k$.

  Conversely, assume that $\mu_k\, A^k - A^0 \succeq 0$. If $\mu_k = 1$, the
  statement is clear, so assume that $\mu_k = \hat{\mu}_k < 1$. Then for
  all $x \in \R^n$:
  \[
    0 \leq x\T \big(\mu_k\, A^k - A^0\big) x = \mu_k\,
    \underbrace{x\T A^k x}_{\geq 0} -
    x\T A^0 x \leq x\T (A^k - A^0) x,
  \]
  which implies that $A^k - A^0 \succeq 0$.
\end{proof}

\begin{lemma}\label{lem:TightenedProblem}
  Let $A^k \succeq 0$ for all $k \in [m]$ and $y \in \R^m$ with
  $y_i \in \{0,1\}$ for all integral variables $i \in I$ and $y_i \geq 0$
  for $i \notin I$. Then
  \[
    A(y) \succeq 0 \quad\Leftrightarrow\quad
    \sum_{k=1}^m \mu_k\, A^k\, y_k - A^0 \succeq 0,
  \]
  where we define $\mu_k = 1$ for $i \notin I$.
\end{lemma}

\todo[inline]{FM: If we tighten the coefficients, we need to scale the
  linear constraints (for example, a trace constraint on~$A(y)$)
  accordingly.}

\begin{proof}
  First assume that
  $\sum_{k=1}^m \mu_k\, A^k\, y_k - A^0 \succeq 0$. Then for every
  $x \in \R^n$
  \[
    0 \leq x\T \bigg(\sum_{k=1}^m \mu_k\, A^k - A^0\bigg) x =
    \sum_{k=1}^m \mu_k\, \underbrace{x\T A^k x}_{\geq 0} -
    x\T A^0 x \leq x\T (\sum_{k=1}^m A^k - A^0) x,
  \]
  which implies that $A(y) \succeq 0$.

  We now assume that $A(y) \succeq 0$. By removing terms with $y_k = 0$ for
  $k \in I$, we can assume that $y_k = 1 $ for all $k \in I$. Thus,
  $\sum_{k=1}^m A^k - A^0 \succeq 0$. If $\mu_k = 1$ for all $k \in [m]$,
  then the statement is directly clear. Therefore assume that there exists
  $k \in I$ with $\mu_k = \hat{\mu}_k < 1$. But then already
  $\mu_k\, A^k - A^0 \succeq 0$. Adding the psd matrices $A^{\ell}$ for
  $\ell \in [m] \setminus \{k\}$ does not change this, which shows the
  claim.
\end{proof}

We now turn to the computation of $\mu_k$.

\begin{lemma}\label{lem:TightenPosDef}
  If $A^k \succ 0$ is positive definite, then
  \[
    \mu_k = \min \big\{ \lambda_{\max}(V^{-\top} A^0 V^{-1}),\, 1\big\}.
  \]
  where $A^k = V\T V$ for some invertible matrix $V \in \R^{n \times n}$.
\end{lemma}

\begin{proof}
  For some matrix $A \in \R^{n \times n}$ and invertible matrix
  $B \in \R^{n \times n}$ we have $A \succeq 0$ if and only if
  $B\T A B \succeq 0$ (see, e.g., Helmberg~\cite[Prop.~1.1.7]{Hel00}).
  Thus, $\mu\, A^k - A^0 \succeq 0$ if and only if
  \begin{align*}
    0 \preceq (V^{-1})^\top (\mu\, A^k - A^0) V^{-1} &= \mu\,
    (V^{-1})^\top A^k V^{-1} - (V^{-1})^\top A^0 V^{-1} \\
    &= \mu\, I - (V^{-1})^\top A^0 V^{-1}.
  \end{align*}
  The eigenvalues of $\mu\, I - B$ for some matrix $B$ are $\mu - \lambda$
  for eigenvalues $\lambda$ of $B$.\footnote{This follows, for example, by the
    general theorem that if $g(t) \in \R[t]$ is a scalar polynomial and
    $\lambda_1, \dots, \lambda_n$ are the eigenvalues of $B$, then the
    eigenvalues of $g(B)$ are $g(\lambda_1), \dots, g(\lambda_n)$, see
    Gantmacher~\cite[Ch.~IV, Thm.~3]{Gan59I}. We here use
    $g(t) = \mu - t = \mu\, t^0 - t^1$. (Alternatively, $(\mu I - B)v =
    \lambda v \Leftrightarrow Bv = (\mu - \lambda)v$, so $v$ is an
    eigenvector for the eigenvalue $\mu - \lambda$.)}  Thus, to make
  $\mu\, I - V^{-\top} A^0 V^{-1}$ psd, we need
  $\mu \geq \lambda_{\max}(V^{-\top} A^0 V^{-1})$.
\end{proof}

If $A^k$ is ``only'' positive semi-definite, one needs other methods that
we discuss now. Note that we want to find the largest $\mu$ such that
$\lambda_{\min}(\mu\, A^k - A^0) = 0$. It is therefore helpful to consider
$f(\mu) = \lambda_{\min}(\mu\, A^k - A^0)$.

The following is inspired by~\cite{Str16,HigSS16} and only stated here for completeness.

\begin{lemma}
  For any two symmetric matrices $A^k$, $A^0 \in \R^{n \times n}$, the function
  $f(\mu) = \lambda_{\min}(\mu\, A^k - A^0)$ is concave.
\end{lemma}

\begin{proof}
  We have for $\alpha \in [0,1]$ and $\mu_1$, $\mu_2 \geq 0$:
  \begin{align*}
    f\big(\alpha \mu_1 + (1 - \alpha) \mu_2\big)
    & = \lambda_{\min}\big((\alpha \mu_1 + (1 - \alpha) \mu_2) A^k - A^0\big)\\
    & = \lambda_{\min}\big(\alpha (\mu_1\, A^k - A^0) + (1 - \alpha) (\mu_2\, A^k - A^0)\big)\\
    & \geq \lambda_{\min}\big(\alpha (\mu_1\, A^k - A^0)\big) + \lambda_{\min}\big((1 - \alpha) (\mu_2\, A^k - A^0)\big)\\
    & = \alpha\, \lambda_{\min}\big(\mu_1\, A^k - A^0\big) + (1 - \alpha)\, \lambda_{\min}\big(\mu_2\, A^k - A^0\big)\\
    & = \alpha\, f(\mu_1) + (1 - \alpha)\, f(\mu_2).
  \end{align*}
  The inequality follows because for a symmetric matrix~$C \in \R^{n \times n}$:
  \[
    \lambda_{\min}(C) = \min_{\norm{x} = 1} x\T C x.
  \]
  Thus, for a symmetric matrix $D \in \R^{n \times n}$
  \begin{align*}
    \lambda_{\min}(C + D) & = \min_{\norm{x} = 1} x\T (C + D) x\\
    & \geq \min_{\norm{x} = 1} x\T C  x + \min_{\norm{x} = 1} x\T D x =
    \lambda_{\min}(C) + \lambda_{\min}(D),
  \end{align*}
  which shows the inequality.
\end{proof}

\noindent
There are at least three choices to compute $\mu_k$:
\begin{itemize}[leftmargin=3ex]
\item One can use bisection within the interval $[0,1]$, checking whether
  $f(\mu) \geq 0$ in each step.
\item One can use Newton's method to compute $\mu_k$ similar
  to~\cite{Str16,HigSS16}.
\item One can solve the generalized eigenvalue problem $\mu A^k x = \lambda
  A^0 x$. It seems to be the case that most algorithms for this problem
  work best when one of the matrices is positive definite. But in this case
  we can apply Lemma~\ref{lem:TightenPosDef}. Otherwise, one can use the
  technique described in~\cite{Str16,HigSS16}. 
\end{itemize}

Note that~\eqref{eq:ComputeHatMu} might be infeasible, e.g., if $A^0$ is
not negative definite and $A^k$ is psd with minimal eigenvalue 0. An
extreme example is if $A^k = 0$, but $A^0 \neq 0$. Thus, the implementation
of these methods have to be safe-guarded against infeasibility.

\begin{question}
  Is there an easy way to determine whether~\eqref{eq:ComputeHatMu} is
  infeasible, e.g., by computing eigenvalues once?  \footnote{For example
    is the following true: If $\mu A^k - A^0$ is not psd for
    $\mu \geq \lambda_{\max}(A^0)/\lambda_{\max}(A^k)$ then~\eqref{eq:ComputeHatMu} is
    infeasible? This would require that $\mu A^k - A^0 \succeq 0$ for all
    $\mu \geq \lambda_{\max}(A^0) / \lambda_{\max}(A^k)$, e.g., if $A^0$ is
    psd and if there exists any $\mu'$ such that $\mu' A^k - A^0 \succeq 0$.}
\end{question}

According to~\cite{Str16,HigSS16}, solving the generalized eigenvalue
problem is fastest, followed by bisection, and then Newton's method. But
note that the problems in~\cite{Str16,HigSS16} are slightly different
($f(\alpha) = \lambda_{\min}(\alpha A^k + (1 - \alpha) A^0)$).

The methods are implemented (using bisection) but none of the instances in
the testset allows a tightening.

\begin{question}
  Can these methods be successful and are they relevant?
\end{question}



% -------------------------------------------------------------------------
\subsection{Bound Tightening}

One can also use the methods in Section~\ref{sec:CoefficientTightening} to
compute bounds on each $y_k$ variable. For this we only need that all $A^k$
are psd, but $y_k$ does not need to be binary. We need the following:

\begin{lemma}
  Let $A^k \succeq 0$ for all $k \in [m]$ and $\ell \in [m]$. Then for
  $y \in \R^m$, $y \geq 0$:
  \[
    A(y) \succeq 0 \quad\Leftrightarrow\quad
    \alpha\, A^{\ell} + \sum_{\substack{k=1\\k \neq \ell}}^m A^k\, y_k - A^0 \succeq 0,
  \]
  where $\alpha = \min\,\{y_{\ell},\hat{\mu}_{\ell}\}$.
\end{lemma}

\todo[inline]{FM: If we tighten the bounds of a variable~$y_k$, we need to
  scale the linear constraints accordingly.}

\begin{proof}
  Observe that if $\alpha = y_{\ell}$, then the statement is trivially
  true. Thus, let $\alpha = \hat{\mu}_k < y_{\ell}$.

  First assume that
  $\alpha\, A^{\ell} + \sum_{k=1,k \neq \ell}^m \hat{\mu}_k\, A^k\, y_k - A^0
  \succeq 0$.  Then for every $x \in \R^n$
  \begin{align*}
    0 & \leq x\T \bigg(\alpha\, A^{\ell} + \sum_{\substack{k=1\\k \neq \ell}}^m A^k\, y_k - A^0\bigg) x =
        \alpha\, \underbrace{x\T A^{\ell} x}_{\geq 0} + \sum_{\substack{k=1\\k \neq \ell}}^m x\T A^k
        x\, y_k - x\T A^0 x\\
      & \leq y_{\ell}\, x\T A^{\ell} x + \sum_{\substack{k=1\\k \neq \ell}}^m x\T A^k x\, y_k - x\T A^0 x
        = x\T A(y)x,
  \end{align*}
  which implies that $A(y) \succeq 0$.

  We now assume that $A(y) \succeq 0$. By definition of $\hat{\mu}_{\ell}$
  we have $\hat{\mu}_{\ell}\, A^{\ell} - A^0 \succeq 0$. Note that
  $A^k \, y_k$ is psd, since $A^k$ is psd and $y_k \geq 0$. Then adding
  these matrices for all for $k \in [m] \setminus \{\ell\}$ does not change
  the property of being psd, which shows the claim.
\end{proof}

From the lemma, we conclude that one can tighten the upper bounds of all
variables $y_{\ell}$ to $\min\, \{u_{\ell}, \hat{\mu}_{\ell}\}$. This is
implemented, but has not tightened any bound in presolving.



% -------------------------------------------------------------------------
\subsection{Other Presolving Methods}

We collect some more techniques here, all of which are currently not implemented.

\paragraph{Removing 0 rows/columns}

If for some $i \in [m]$ we have $A_{ij}^k = 0$ for all $j \in [n]$ and
$k \in [m]_0$, then row/column $i$ is not used and can be removed from the
matrices $A^k$.

\paragraph{Dual Fixing}

If for some $k \in [m]$ the matrix $A^k$ is psd and disjoint from the rest
(i.e., $A^k$ has no common nonzero with the other matrices), depending on
the objective coefficient one can fix $y_k$ to its upper bound.

\paragraph{Bounds for Rank 1 Motivated Structures}

If the constraints contain
\[
  \begin{pmatrix}
    1 & x\T \\
    x & X
  \end{pmatrix}
  \succeq 0,
  \quad
  \ell \leq x \leq u,
\]
then Nohra et al.~\cite{NohRS20} propose the following inequalities for all
$i \in [n]$:
\begin{align*}
  X_{ii} & \geq 2\, \ell_i x_i - \ell_i^2,\\ 
  X_{ii} & \geq 2\, u_i x_i - u_i^2.
\end{align*}
For example, the validity of the first type can be seen as follows:
\[
  (x_i - \ell_i)^2 \geq 0 \quad\Leftrightarrow\quad x_i^2 \geq 2\,
  \ell_i x_i - \ell_i^2.
\]
Combining with the $2 \times 2$-minor inequality $X_{ii} - x_i^2 \geq 0$
yields the inequality. The inequality $X_{ii} \geq 2\, u_i x_i - u_i^2$
arises similarly.

The given inequalities are not yet implemented, but the results
in~\cite{NohRS20} suggest that they might be usefull if the LP-relaxation
is solved.

The above structure arises from relaxing quadratic problems. In this case,
additionally $X = x x\T$ and the rank of $X$ has to be 1. Then additional
inequalities have to hold, for example McCormick inequalities.

\begin{remark}
  This can be generalized to the more interesting case of a
  constraint of the form
  \[
  \begin{pmatrix}
    t & x\T \\
    x & X
  \end{pmatrix}
  \succeq 0,
  \quad
  \ell \leq x \leq u,\quad \ell_t \leq t \leq u_t,
\]
where~$t$ is a scalar variable. Constraints of this particular form
appear, e.g., in truss topology design and cardinality least-squares
problems. The same argument as above yields the two inequalities
\begin{align*}
  X_{ii} &\geq \frac{2 \ell_i x_i - \ell_i^2}{u_t} \\
  X_{ii} &\geq \frac{2 u_i x_i - u_i^2}{u_t}
\end{align*}
Assume that~$\ell_i < 0$ and ~$u_i > 0$. Then, these inequalities can be
non-trivial, that is, the right-hand-side is nonnegative,
if~$x_i \leq \ell_i/2$ and~$x_i \geq u_i/2$, respectively. Additionally, by
replacing~$x_i$ by its lower bound~$\ell_i$, the first inequality can be
used for propagation:
\begin{align*}
  X_{ii} &\geq \frac{2 \ell_i u_i - \ell_i^2}{u_t},
\end{align*}
where the right-hand-side is strictly positive, if $\ell_i > u_i/2$, $u_t >
0$ and either~$u_i > 0$ or~$\ell_i$, $u_i < 0$.

These inequalities are currently not implemented, but it may be worth
testing them on different types of problems with LP- or SDP-solving to see
if they have any impact.
\end{remark}


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\section{Useless or Already Implied Methods}


% -------------------------------------------------------------------------
\subsection{Implications of Primal Bounds}

The implications in the following results are automatically exploited for
linear constraints in problems of the form~\eqref{MISDP-P}. The argument
is standard argument, but we repeat it to see whether it can be exploited
somewhere.

\begin{lemma}
  Let $A \in \R^{n \times n}$, $\beta \in \R$ and define
  \[
    \mathcal{X} \define \{ X \in \R^{n\times n} \suchthat \skal{A}{X} \leq
    \beta,\; L_{ij} \leq X_{ij} \leq U_{ij}\; \forall (i,j) \in [n]^2\}.
  \]
  Then for any $X \in \mathcal{X}$ the inequality
  \[
    X_{st} \leq \frac{\beta - \alpha_{st}}{A_{st}}
  \]
  holds for all $(s,t) \in [n]^2$, where
  \begin{align*}
    & \alpha_{st} \define \sum_{(i,j) \in S^-_{st}} A_{ij} u_{ij} - \sum_{(i,j) \in S^+_{st}} A_{ij} \ell_{ij},\\
    & S^+_{st} = \{(i,j) \in [n]^2 \setminus \{(s,t)\} \suchthat A^k_{ij} > 0\},\\
    & S^-_{st} = \{(i,j) \in [n]^2 \setminus \{(s,t)\} \suchthat A^k_{ij} < 0\}.
  \end{align*}
\end{lemma}

\begin{proof}
  For any~$X \in \mathcal{X}$ we have
  \[
    \sum_{(i,j) \in [n]^2} A_{ij} X_{ij} \leq \beta
    \quad\Leftrightarrow\quad A_{st} X_{st} \leq \beta - \sum_{(i,j) \in [n]^2
      \setminus \{(s,t)\}} A_{ij} X_{ij}.
  \]
  Then
  \[
    A_{st} X_{st} \leq \beta - \sum_{(i,j) \in S^-_{st}} A_{ij} u_{ij} - \sum_{(i,j) \in S^+_{st}} A_{ij} \ell_{ij}.
  \]
  This yields the claim.
\end{proof}


%-------------------------------------------------------------------------
\subsection{Gershgorin-based Presolving}

Gershgorin's circle theorem says that each eigenvalue of a matrix
$A \in \R^{n \times n}$ lies in (at least) one of the Gershgorin discs
\[
  D_i \define \Big\{z \in \C \suchthat \abs{z - a_{ii}} \leq \sum_{j=1, j \neq
    i}^n \abs{a_{ij}} \Big\}.
\]
Thus, given a vector $y \in \R^m$, for $A(y) \succeq 0$ to hold, we need
that
\[
  \bigcup_{i=1}^n \bigg\{z \in \C \suchthat \abs{z - A(y)_{ii}} \leq \sum_{j=1, j \neq
    i}^n \abs{A(y)_{ij}}\bigg\} \cap \{z \in \C \suchthat \Re(z) \geq 0\}
  \neq \varnothing.
\]
But since we need to have $A(y)_{ii} \geq 0$ for all $y$ with
$A(y) \succeq 0$, the above condition is always fulfilled: The center
points of the circles are always in the nonnegative complex halfplane.

\begin{remark}
  This idea can be used to detect a cut off for an SDP-constraint that
  should have rank 1. Recall that a positive semidefinite matrix has rank 1
  if and only if all of its 2 by 2 minors are zero. This implies that the
  smallest eigenvalue of each 2 by 2 submatrix needs to be 0. Thus, if
  there exist two indices~$i$, $k$, so that
  \begin{align*}
    0 \notin D_i,\quad 0 \notin D_k, \quad D_i \cap D_k = \varnothing,
  \end{align*}
  then the current node can be cut off, since a minimal eigenvalue of 0 is
  not obtainable for the 2 by 2 submatrix indexed by~$(i,k)$.

  In the case of a dual SDP-constraint~$A(y) \geq 0$, the Gershgorin
  discs~$D_i$, $D_k$ do not contain 0, if the lower bounds~$\ell_i$,
  $\ell_k$ of the entries~$A(y)_{ii}$ and~$A(y)_{kk}$ are strictly
  positive, and if the radius of the discs is strictly smaller
  than~$\ell_i$, $\ell_k$, i.e.,
  \begin{align*}
    \ell_i > 0,\quad \ell_k > 0, \qquad \sum_{j\neq i} \abs{A(y)_{jj}} <
    \ell_i, \quad \sum_{j\neq k} \abs{A(y)_{jj}} < \ell_k.
  \end{align*}
  Currently, this is not implemented for SDP-constraints with an additional
  rank-1 constraint.
\end{remark}

% -------------------------------------------------------------------------


% -------------------------------------------------------------------------
\subsection{SOCP-inspired Presolving}

Consider a second order cone constraint $(x,t) \in L_{n+1}$, where
\[
  L_{n+1} \define \Big\{x \in \R^{n+1} \suchthat \sqrt{x_1^2 + \dots + x_n^2}
  \leq x_{n+1}\Big\}
\]
is the Lorentz cone.  Presolving/propagation of
$\sqrt{x_1^2 + \dots + x_n^2} \leq t$ with bounds $\ell \leq x \leq u$
yields for some $k \in [n]$:
\begin{equation}\label{eq:SOCPpre}
  x_k^2 \leq t^2 - \sum_{\substack{i=1\\ i \neq k}}^n x_i^2 \leq t^2 -
  \sum_{\substack{i=1\\ i \neq k}}^n \gamma_i \leq t^2,\qquad
  \gamma_i \define \min_{\gamma \in [\ell_i, u_i]} \gamma^2.
\end{equation}
Note that $\ell_i < 0 < u_i$ is possible. One main application is the case
in which there is an upper bound on~$t$. We would like to have a similar
presolving (propagation) for SDPs -- at least if we would write an SOCP as
an SDP, but possibly for more general cases.

Indeed, SOCP is special case of SDP, because:
\[
  (x,\sqrt{t}) \in L_{n+1} \quad\Leftrightarrow\quad
  \tilde{X} \define
  \begin{pmatrix}
    t & x\T\\
    x & I_n
  \end{pmatrix}
  \succeq 0.
\]
To see this, we need the following result adapted
from~\cite[Section~A.5.5]{BoyV09}. Let
\[
  D = \begin{pmatrix}
    A & B\T\\
    B & C
  \end{pmatrix}
\]
with a positive definite $C \succ 0$. Then $D \succeq 0$ if and only if
$S = A - B\T C^{-1} B \succeq 0$ and $A \succeq 0$\footnote{Note that the
  condition $A \succeq 0$ is missing in~\cite{BoyV09}.}. For $D = \tilde{X}$, we
get: $\tilde{X} \succeq 0$ if and only if $t \geq 0$, $t - x\T x \geq
0$. The latter is equivalent to $(x,\sqrt{t}) \in L_{n+1}$.

Now consider a more general matrix
\begin{equation}\label{eq:GeneralPSD}
  \begin{pmatrix}
    t & x\T\\
    x & X
  \end{pmatrix}
  \succeq 0
\end{equation}
with an upper bound $\bar{t}$ on $t$. This seems to be the most natural
generalization of the SOCP case.

Taking $2 \times 2$ minors for some $i \in [n]$, we get
$t X_{ii} - x_i^2 \geq 0$. Thus, $x_i^2 \leq \bar{t} X_{ii}$, which leads
to
\[
  - \sqrt{\bar{t} X_{ii}} \leq x_i \leq \sqrt{\bar{t} X_{ii}}.
\]
Note that this is covered by the bounds considered in
Section~\ref{sec:2by2Minors}. In the SOCP case, we obtain
$x_i \leq \sqrt{\bar{t}}$, which only provides the weakest upper bound
in~\eqref{eq:SOCPpre}.

Thus, one question is whether one can derive strengthened bounds. This is
currently unclear, but it does not seem to be possible.

\paragraph*{Some Ideas}
As one idea, one can consider generalized Schur
polynomials (since $X$ is not necessarily positive definite, one needs the
generalization), which we adapt from~\cite[Section~A.5.5]{BoyV09}: Consider
again the matrix $D$ from above. Then
\[
  D \succeq 0 \quad\Leftrightarrow\quad
  A \succeq 0,\; (I - AA^{+})B\T = 0,\; C - B A^{+} B\T \succeq 0,
\]
where $A^+$ is the pseudo-inverse of $A$. Reversing the roles of $A$ and
$C$ we get:
\[
  D \succeq 0 \quad\Leftrightarrow\quad
  C \succeq 0,\; (I - CC^{+})B = 0,\; A - B\T C^{+} B \succeq 0.
\]
Applied to~\eqref{eq:GeneralPSD}, we obtain from the first:
\begin{equation}\label{eq:GenSchur1}
  t \geq 0,\; (1 - t t^{-1}) x\T = 0,\; X - x t^{-1} x\T \succeq 0,\; 
\end{equation}
if $t > 0$ and from the second:
\begin{equation}\label{eq:GenSchur2}
  X \succeq 0,\; (I - XX^{+})x = 0,\; t - x\T X^{+} x \succeq 0.
\end{equation}

Using
$X - \tfrac{1}{t} x x\T \succeq 0 \Leftrightarrow t X - x x\T \succeq 0$
from~\eqref{eq:GenSchur1}, we get for the $2 \times 2$-minors with $i$,
$j \in [n]$, $i \neq j$:
\begin{align*}
  & (t\, X_{ii} - x_i^2) (t\, X_{jj} - x_j^2) -  (t\, X_{ij} - x_i\, x_j)^2 \geq 0
  \\
  \Leftrightarrow\quad & t \, X_{ij} - x_i\, x_j \leq \sqrt{t\, X_{ii} - x_i^2} \sqrt{t\, X_{jj} - x_j^2}
  \\
  \Leftrightarrow\quad & X_{ij} \leq \frac{1}{t} \bigg(\sqrt{t\, X_{ii} - x_i^2}
                         \sqrt{t\, X_{jj} - x_j^2} + x_i\, x_j\bigg).
\end{align*}
This, for instance, yields
\[
  X_{ij} \leq \sqrt{U_{ii}} \sqrt{U_{jj}} + \tfrac{1}{t} x_i\, x_j.
\]
which does not seem to help. If we would know that $X_{ij} = x_i\, x_j$,
i.e., $X$ is rank-1, then we obtain
\[
  X_{ij} \leq \frac{\sqrt{t\, X_{ii} - x_i^2} \sqrt{t\, X_{jj} - x_j^2}}{t-1}.
\]


\begin{question}
  Can any of this be made useful?
\end{question}


% -------------------------------------------------------------------------
\section{Specialized Arguments for Computing the RIP}

\subsection{Sparse Principal Component Analysis}

Consider the sparse principal component analysis (PCA) problem
\[
  \max\; \{\Norm{Ax}{2}^2 \suchthat \Norm{x}{2}^2 = 1,\; \Norm{x}{0} \leq k\}. 
\]
with $A \in \R^{m \times n}$.  Let $x^*$ be an optimal solution and
$S = \supp(x^*)$ be its support with $k \define \Norm{x}{0}$. Consider the
submatrix~$A_S \in \R^{m \times k}$ indexed by columns in $S$. Then
$\tilde{A} = A_S\T A_S^{\phantom{T}} \in \R^{k \times k}$ is symmetric positive
semidefinite. By the Rayleigh-Ritz theorem (see
Helmberg~\cite[Thm.~A.0.4]{Hel00}) we have
\[
  \max_{y \in \R^k} \; \{\Norm{\tilde{A}y}{2}^2 \suchthat \Norm{y}{2}^2 =
  1\} = \lambda_{\max}(\tilde{A}),
\]
i.e., the sparse PCA problem looks for a $k$-sparse vector yielding the
largest eigenvalue of the supporting submatrix. A similar statement holds
for minimization variant and the minimal eigenvalue.

A formulation for the sparse PCA problem is
\begin{subequations}\label{eq:SPCA}
  \begin{align}
    \max \quad & \skal{A\T A}{X} \\
    \text{s.t.} \quad & \tr(X) = 1, \\
    & -z_j \leq X_{ij} \leq z_j \quad \text{for } i,\; j \in [n],\label{eq:SPCAcouple} \\
    & \sum_{i=1}^n z_i \leq k,\\
    & X \succeq 0, \\
    & z \in \{0,1\}^n.
  \end{align}
\end{subequations}
In~\cite{GalP16} (and~\cite{LiX20}) it is proved that there exists an
optimal rank-1 solution~$X^*$. Thus, $X^* = x^* (x^*)\T$ for some
$x^* \in \R^n$ with $\Norm{x^*}{0} \leq k$. Let $S = \supp(x^*)$. Then
$x^*_S$ is an eigenvector for a maximal eigenvalue of
$A_S\T A_S^{\phantom{T}}$.

\begin{question}
  Since $X_{ii} = 0$ implies $X_{ij} = 0$ for all $j \in [n]$, it would
  suffice to only consider the inequalities $-z_i \leq X_{ii} \leq x_i$ for
  all $i \in [n]$ in~\eqref{eq:SPCAcouple}. Can we detect that the other
  inequalities are implied and remove them? Does this speed up the solution
  process?
\end{question}

\begin{question}
  One idea is to perform spatial branching on the diagonal entries
  $X_{ii}$. Does this help to solve the problem?
\end{question}

\begin{remark}
  For the case of~$k=2$, an optimal solution~$X$ consists of a single 2 by
  2 nonzero submatrix, and all other entries are 0. Let
  \begin{align*}
    \tilde{X} =  \begin{pmatrix}
      X_{11} & X_{21} \\
      X_{21} & X_{22}
    \end{pmatrix}
  \end{align*}
  be a 2 by 2 submatrix of~$X$. The trace constraint then
  reads~$X_{11} + X_{22} = 1$, and the psd constraint~$X \succeq 0$
  simplifies to~$X_{11},\, X_{22} \geq 0$, and $X_{11}X_{22}\geq
  X_{21}^2$. If~$X_{21} = 0$, then either~$X_{11} = 1$ or~$X_{22} = 1$,
  depending on their coefficients in the objective function.
  If~$X_{21} \neq 0$, aggregate~$X_{22} = 1 - X_{11}$. In this case, the
  solution set is given by a disc centered at $(1/2,\,0)$ with
  radius~$1/2$, where~$X_{11}$ is the first coordinate and~$X_{21}$ the
  second. Optimizing in direction of the objective function yields an
  optimal solution.
\end{remark}

% -------------------------------------------------------------------------
\subsubsection{Complete Description of Feasible Set}

One interesting question is whether one can describe the convex hull of the
feasible set. For this define
\begin{align*}
  S = \Big\{(X,z) \in S^n_+ \times [0,1]^n \suchthat & \tr(X) = 1,\; \sum_{i=1}^n z_i \leq k,\;     0 \leq X_{ii} \leq z_j \;\forall\, i \in [n],\\
                                                     & -\tfrac{1}{2} z_j \leq X_{ij} \leq \tfrac{1}{2} z_j \;\forall\, i \neq j \in [n] \Big\},
\end{align*}
which is the strongest relaxation that we currently have. Its integer hull is
\begin{align*}
  S_I \define \conv \Big\{& (X,z) \in S^n_+ \times \{0,1\}^n \suchthat \tr(X) =
                1,\; \sum_{i=1}^n z_i \leq k,\; 0 \leq X_{ii} \leq z_j \;\forall\, i \in [n] \Big\}.
\end{align*}
In general, we have $S \neq S_I$: If $k = 1$, then all off-diagonals have
to be zero in $S_I$, but not necessarily in~$S$. Moreover, consider
$2 \leq k \leq n$. Then for
\[
  z = \begin{pmatrix}
    \tfrac{1}{k} \\
    \vdots\\
    \tfrac{1}{k}
  \end{pmatrix},\quad
  X \define
  \begin{pmatrix}
    \tfrac{1}{k} & \dots, & \tfrac{1}{k}\\
    \vdots & \ddots & \vdots\\
    \tfrac{1}{k} & \dots & \tfrac{1}{k}
  \end{pmatrix},
\]
we have $(X,z) \in S$, but $(X,z) \notin S_I$.

Indeed, it is NP-hard to optimize over $S_I$ -- this is goal of the
RIP-problem.


% -------------------------------------------------------------------------
\paragraph{Linear Case}

It might be instrumential to look at the following linear analogue:
\[
S^k \define \{(x,z) \in \R_+^n \times [0,1]^n \suchthat \sum_{i=1}^n x_i =
1,\; \sum_{i=1}^n z_i = k,\; 0 \leq x_i \leq z_i \;\forall\, i \in [n]\}.
\]
As before, $S^k_I$ denotes the integer hull
$S^k_I \define \conv\big(S^k \cap (\R^n \times \{0,1\}^n)\big)$.

\begin{theorem}
  $S^k = S^k_I$, i.e., $S^k$ is integral.
\end{theorem}

\begin{proof}
  The constraint matrix of $S^k$ with respect to the equations and the
  inequalities $x - z \leq 0$ can be written as follows:
  \[
    M \define
    \begin{pmatrix}
      1 & \dots & 1 & 0 & \dots & 0 \\
      0 & \dots & 0 & 1 & \dots & 1 \\
      1 &       &   & -1 &      &   \\
        & \ddots&   &    & \ddots & \\
      0 &       & 1 & 0 &      & -1
    \end{pmatrix}.
  \]
  This matrix is totally unimodular. This can be seen by multiplying the
  first row with $-1$. Then we have exactly one $+1$ and $-1$ in each
  column. Thus, $M$ is the node-arc indicence matrix of a directed graph
  and hence TU. To take care of the variables bounds, one would add
  identity matrices to $M$, which do not change the property of being
  TU. Moreover, the right hand sides are integral and thus the polytope is
  integral.
\end{proof}

As a consequence, when solving
\[
  \min\, \{ c\T x + d\T z \suchthat (x,z) \in S^k \}
\]
one can even chose the $x$-variables to be integral, i.e., in 0 or 1, in an
optimal solution.

\begin{remark}
  Aghezzaf and Wolsey~\cite{AghW92} show the following: Let
  $T \subseteq \R^p_+ \times \{0,1\}^n$, $P = \conv(T)$, and
  \[
  P_k \define \{ (x,z) \in P \suchthat \sum_{i=1}^n z_i = k\}.
  \]
  For $k = 0, \dots, n$ define
  \[
  g_k \define \min\,\{c\T x + d\T z \suchthat (x,z) \in P_k,\; z \in
  \{0,1\}^n\}.
  \]
  Then they show that
  \[
  \min\,\{c\T x + d\T z \suchthat (x,z) \in P_k\}
  \]
  has an optimal solution in $T$ if and only if $g_{k+1} - g_k$ is
  non-decreasing for $k = 1, \dots, n - 1$.
  % Alternatively, if for some $\mu$
  % \[
  % \min\, \{c\T x + d\T z - \mu \sum_{i=1}^n z_i \suchthat (x,z) \in P\}
  % \]
  % has integral optimal solutions $(x^1,z^1)$ and $(x^2,z^2)$ with
  % $\ones\T z^1 = k_1$ and $\ones\T z^2 = k_2$ with $k_1 \leq k_2 - 2$, then
  % there exists an alternative integral optimal solution with $\ones\T z = k$ and
  % $k_1 < k < k_2$. Note that one can integrate the $\mu$-term into $d$.

  In our situation we have
  \[
  T = \{(x,z) \in \R_+^n \times \{0,1\}^n \suchthat \sum_{i=1}^n x_i = 1,\;
  0 \leq x_i \leq z_i \;\forall\, i \in [n]\}.
  \]
  The convex hull of $T$ is simply obtained by relaxing $z$ to be in $[0,1]^n$:
  \[
  P = \{(x,z) \in \R_+^n \times [0,1]^n \suchthat \sum_{i=1}^n x_i = 1,\;
  0 \leq x_i \leq z_i \;\forall\, i \in [n]\}.
  \]
  Thus, $P_k = S^k$.

  Then Aghezzaf and Wolsey consider lot sizing problems for given demands $D_1,
  \dots, D_n$:
  \begin{align*}
    \min\quad & c\T x + d\T z\\
    & \sum_{i=1}^t x_i \geq \sum_{i=1}^t D_i\quad \forall \, t = 1, \dots, n - 1,\\
    & \sum_{i=1}^n x_i = \sum_{i=1}^n D_i,\\
    & 0 \leq x \leq M z,\\
    & z \in \{0,1\}^n.
  \end{align*}
  If $D_i = 0$, $i = 1, \dots, n-1$, $D_n = 1$, and $M = 1$, we obtain our problem.

  Aghezzaf and Wolsey show that the optimum is attained in an integral
  point for each $k$ if $c_1 \geq c_2 \geq \dots \geq c_n$. But the proof
  is not completely clear.
\end{remark}

\subsubsection{Nonnegativity by the Perron-Frobenius Theorem}

We apply the Perron-Frobenius theorem. To this end, we define a matrix $A$ to
be \emph{reducible} if there exists a permutation matrix $P$ such that
\[
  P\T A P = \begin{pmatrix} B & C \\ 0 & D \end{pmatrix}.
\]
Otherwise it is \emph{irreducible}. We have the following:

\begin{theorem}[{Frobenius Theorem, see Gantmacher~\cite[Chapter 2, Thm.~2]{Gan59II}}]\label{thm:PerronFrobenius}
  An irreducible nonnegative matrix $A$ has a positive maximal eigenvalue
  $\lambda > 0$. The corresponding eigenvectors have positive entries.
\end{theorem}

For general (i.e., also reducible) matrices the following holds:
\begin{theorem}[{Gantmacher~\cite[Chapter 2, Thm.~3]{Gan59II}}]\label{thm:PerronFrobeniusNonnegative}
  A nonnegative matrix $A$ has a nonnegative maximal eigenvalue
  $\lambda \geq 0$. The corresponding eigenvectors have nonnegative
  entries.
\end{theorem}

Moreover, Gantmacher proves the following.
\begin{lemma}[{\cite[Chapter 2, Remark 3]{Gan59II}}]\label{lemma:PFNegative}
  An irreducible nonnegative matrix $A$ cannot have two linearly
  independent nonnegative eigenvectors.
\end{lemma}
This implies that all eigenvectors to all other eigenvalues cannot be nonnegative!
For oscillaritory matrices some more can be said about the signs, see
\cite[Chapter 2, Theorem~13]{Gan59II}.

\begin{lemma}
  If $A_S \geq 0$, then $x^* \geq 0$.
\end{lemma}

\begin{proof}
  Since $A_S \geq 0$, Theorem~\ref{thm:PerronFrobeniusNonnegative} implies
  that $A_S$ has a maximal eigenvalue with corresponding \emph{nonnegative}
  eigenvector. This eigenvector is unique up to scaling.
\end{proof}

Thus, one can restrict $X \geq 0$ and write
\[
  0 \leq X_{ij} \leq z_j \quad \text{for } i,\; j \in [n]
\]
instead of~\eqref{eq:SPCAcouple}.

\subsubsection{Two Valid SOCP Constraints}

Li and Xie~\cite{LiX20} show that the following inequalitiy is valid for
sparse PCA:
\[
  \sum_{j \in [n]} X_{ij}^2 \leq X_{ii}\, z_i \quad\text{for all } i \in [n].
\]
This holds, since in this case on can assume that $X = x x\T$ and $\sum_{j=1}^m x_j^2 =
1$.\footnote{Note that this might cut off optimal solutions that do not
  have rank 1.} Thus,
\[
  \sum_{j \in [n]} X_{ij}^2 \leq \sum_{j \in [n]} x_i^2\, x_j^2 = x_i^2
  \leq z_i\, X_{ii}.
\]
Note that these inequalities can be reformulated by subtracting $X_{ii}^2$
from both sides to yield
\[
  \sum_{\substack{j \in [n]\\ j \neq i}} X_{ij}^2 \leq z_i\, (X_{ii} - X_{ii}^2).
\]


These inequalities are SOCP representable as
\begin{align*}
  & (X_{i1}, \dots, X_{in}, \tfrac{X_{ii} - z_i}{2}, \tfrac{X_{ii} + z_i}{2}) \in L_{n+2},\\
  \Leftrightarrow\quad & \sum_{j=1}^n X_{ij}^2 + \tfrac{1}{4}(X_{ii}^2 - 2\,
                    z_i\, X_{ii} + z_i^2) \leq \tfrac{1}{4}(X_{ii}^2 + 2\, z_i\, X_{ii} + z_i^2)\\
  \Leftrightarrow\quad & \sum_{j=1}^n X_{ij}^2 \leq z_i\, X_{ii}.
\end{align*}
Formulated as an SDP, we obtain
\[
  \begin{pmatrix}
    \tfrac{X_{ii} + z_i}{2} & X_{i1} & \dots & X_{in} & \tfrac{X_{ii} - z_i}{2}\\
    X_{i1} & 1 &  & 0 & 0\\
    \vdots & & \ddots & & \vdots\\
    X_{in} & &    & 1 & 0\\
    \tfrac{X_{ii} - z_i}{2} & & \dots & 0 & 1
  \end{pmatrix}
  \succeq 0,
\]
for all $i \in [n]$.  These SDP blocks can in principle be added
to~\eqref{eq:SPCA}. Alternatively, one can encode the weaker inequality
$X_{ij}^2 \leq z_i\, X_{ii}$ as
\[
  \begin{pmatrix}
    z_i & X_{ij}\\
    X_{ij} & X_{ii}
  \end{pmatrix}
  \succeq 0,
\]
but this would require quadratically many SDP-constraints.

Moreover, since $X_{ii} \leq x_i$, we have the weaker
$\sum_{j=1}^n X_{ij}^2 \leq z_i^2$, which is equivalent to
\[
  \begin{pmatrix}
    z_i & X_{i1} & \dots & X_{in} \\
    X_{i1} & 1 &  & 0 \\
    \vdots & & \ddots & \vdots \\
    X_{in} & &    & 1 \\
  \end{pmatrix}
  \succeq 0.
\]

\begin{question}
  Can these inequalities be useful within SCIP-SDP?
\end{question}

\begin{question}
  Is there any inequality that exploits the fact that $z_i^2 = z_i$ for
  feasible solutions?
\end{question}

A second type of inequality of Li and Xie~\cite{LiX20} is
\[
  \bigg(\sum_{j \in [n]} \abs{X_{ij}}\bigg)^2 \leq k\, X_{ii}\, z_i\quad
  \forall\, i \in [n].
\]
These inequalities seem to be nontrivial
\footnote{Note that the inequality $\sum_{j \in [n]} \abs{X_{ij}} \leq k$ is trivial,
since $\abs{X_{ij}} \leq z_j$ and thus
\[
  \sum_{j \in [n]} \abs{X_{ij}} \leq \sum_{j \in [n]} z_j \leq k.
\]
Similarly, $\sum_{j \in [n]} \abs{X_{ij}} \leq z_i$ holds, but it seems
that $\sum_{j \in [n]} \abs{X_{ij}} \leq X_{ii}\, z_i$ does not hold in general.}
and follow, because
\[
  \sum_{j \in [n]} \abs{X_{ij}} = \sum_{j \in [n]} \abs{x_i} \abs{x_j} =
  \abs{x_i} \sum_{j \in [n]} \abs{x_j} \leq \abs{x_i} \sqrt{k},
\]
where we used that $\norm{x}_1 \leq \sqrt{k}$ if $\norm{x}_2 = 1$ and
$\norm{x}_0 \leq k$. Moreover, $\abs{x_i} = \sqrt{X_{ii}} = \sqrt{X_{ii}
  z_i}$, because if $z_i = 0$ then $X_{ii} = 0$.

These inequalities are SOC representable, but one needs to express the
absolute values in $\sum_{j \in [n]} \abs{X_{ij}}$, e.g., by variable
splitting.

\subsection{Minimization Variant}

Now consider the lower problem
\[
  \min_{y \in \R^k} \; \{\Norm{\tilde{A}y}{2}^2 \suchthat \Norm{y}{2}^2 =
  1\} = \lambda_{\min}(\tilde{A}).
\]
Unfortunately, by Lemma~\ref{lemma:PFNegative}, a vector for an eigenvalue
that is not equal to the (unique) maximal eigenvalue always has positive
and negative entries.

Nevertheless if the matrix $A\T A$ is \emph{positive definite}, we can
solve the maximization version in~\eqref{eq:SPCA} where $A\T A$ is replaced
by $(A\T A)^{-1}$. This holds since $\lambda$ is an eigenvalue of $A$ if
and only if $1/\lambda$ is an eigenvalue of $A^{-1}$. This transformation
might have the advantage that the maximization version is sometimes easier
to solve, but this might only hold for nonnegative matrices $A$. Note,
however, that $(A\T A)^{-1}$ is very unlikely to be nonnegative, so we
cannot benefit from the application of the Perron-Frobenius theorem here.

\begin{question}
  Can the transformation help to speed up the solution process? How often
  are the matrices positive definite?
\end{question}


\begin{small}
  \bibliographystyle{abbrv}
  \bibliography{MISDPpresolving.bib}
\end{small}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
