\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[lined,ruled,noend,linesnumbered]{algorithm2e}
\usepackage[color=red!50!blue!50,textsize=footnotesize,textwidth=4cm]{todonotes}
\usepackage[breaklinks,colorlinks,citecolor=blue,linkcolor=blue]{hyperref}
% \usepackage[tmargin=2.5cm,bmargin=2.5cm,lmargin=2.8cm,rmargin=2.8cm]{geometry}
\usepackage{cleveref}
\usepackage{xspace}

% operators
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\relint}{relint}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\renewcommand{\Re}{\text{Re}}

% macros
\newcommand{\suchthat}{\,:\,}
\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\card}[1]{\lvert{#1}\rvert}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\define}{\coloneqq}
\newcommand{\enifed}{\eqqcolon}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\Norm}[2]{\lVert{#1}\rVert_{#2}}
\newcommand{\skal}[2]{\langle{#1},{#2}\rangle}
\newcommand{\order}[1]{O\left({#1}\right)}
\newcommand{\T}{^{\top}}

% sets
\newcommand{\R}{\mathds{R}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\C}{\mathds{C}}
\newcommand{\E}{\mathds{E}}
\newcommand{\ones}{\mathds{1}}
\renewcommand{\P}{\mathds{P}}
\renewcommand{\E}{\mathds{E}}

% environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}

% adjust itemize
\setlist[itemize]{topsep=0.5ex,partopsep=0ex,parsep=0ex,itemsep=0.5ex}

% other defines
\newcommand{\MP}[1]{\todo{#1}}
\newcommand{\MPin}[1]{\todo[inline]{#1}}


% title
\title{Presolving for\\ Mixed-Integer Semidefinite Optimization}
\author{Marc E. Pfetsch\thanks{Department of Mathematics, TU Darmstadt, Germany}}
\date{\today}

% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\begin{document}

\maketitle

\section{Introduction}

We consider mixed-integer semidefinite programs (MISDP) of the following
form:
\begin{equation}\label{MISDP}
  \begin{aligned}
    \inf \quad & b\T y \\
    \text{s.t.} \quad & \sum_{i=1}^m A^i\, y_i - A^0 \succeq 0, \\
    & \ell_i \leq y_i \leq u_i && \forall\, i \in [m], \\
    & y_i \in \Z && \forall\, i \in I,
  \end{aligned}
\end{equation}
with symmetric matrices $A^i \in \R^{n \times n}$ for
$i \in [m]_0 \define \{0, \dots, m\}$, $b \in \R^m$,
$\ell_i \in \R \cup \{- \infty\}$, $u_i \in \R \cup \{\infty\}$ for all
$i \in [m] \define \{1, \dots, m\}$. The set of indices of integer
variables is given by $I \subseteq [m]$.

We also consider the following form -- often called ``primal'' form:
\begin{equation}\label{MISDP-P}
  \begin{aligned}
    \inf \quad & \skal{A^0}{X} \\
    \text{s.t.} \quad & \skal{A^i}{X} = b_i && \forall\, i \in [m] \\
    & L_{ij} \leq X_{ij} \leq U_{ij} && \forall\, i,\,j \in [n], \\
    & X_{ij} \in \Z && \forall\, i,\,j \in I \times I,\\
    & X \succeq 0.
  \end{aligned}
\end{equation}
Here the bounds are given by $L_{ij} \in \R \cup \{- \infty\}$, $U_{ij} \in \R \cup \{\infty\}$ for all
$i$, $j \in [m]$.

In this manuscript we try to derive presolving methods for the above two
problems. For instance we try to generalize the presolving techniques from
the MIP to the MISDP case. For instance, we can consider the techniques
demonstrated by Savelsbergh~\cite{Sav94}.

\paragraph{Notation}

We use the notation $A(y) \define \sum_{i=1}^m A^i\, y_i - A^0$ for $y \in \R^m$.


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\section{Possibly Useful Procedures}

One basic argument that we will use repeatedly is the following. Consider a
positive semidefinite (psd) matrix $A \in \R^{n \times n}$. Then the
diagonal entries are nonnegative (since $0 \leq e_i\T A e_i =
A_{ii}$). Then taking the $2 \times 2$-minor for $i$ and $j \in [n]$, we
get $A_{ii}\, A_{jj} - A_{ij}^2 \geq 0$. This implies
\begin{equation}\label{eq:TwoByTwoMinorInequality}
  \abs{A_{ij}} \leq \sqrt{A_{ii} A_{jj}} \leq \frac{A_{ii} + A_{jj}}{2} \leq \max\,\{A_{ii},
  A_{jj}\},
\end{equation}
where we used inequality of geometric and arithmetic means for the second inequality.

This has the following two implications (see, e.g., Observation 1.1.5 and
1.1.6 in Helmberg~\cite{Hel00}):
\begin{enumerate}
\item There the always exists a diagonal entry of maximal absolute value,
  i.e., there exists $r \in [n]$ with
  \[
    A_{rr} = \max\, \{\abs{A_{ij}} \suchthat i,\,j \in [n]\}.
  \]
\item If $A_{ii} = 0$ then $A_{ij} = 0$ for all $j \in [n]$.
\end{enumerate}

\begin{question}
  Is the second condition enforced somehow -- in the sense that the bounds
  are fixed? For example if we branch variables in~\eqref{MISDP} such that
  a diagonal entry of~$A(y)$ becomes 0, are the other entries in the column and row
  fixed to 0 as well?
\end{question}


% -------------------------------------------------------------------------
\subsection{Implied Linear Inequalities}

The following inequalities are known from the literature, see
Gally~\cite{Gal19}, and can be added as linear inequalities w.r.t.\
\eqref{MISDP}:
\begin{itemize}
\item Any feasible $y \in \R^m$ satisfies $A(y) \succeq 0$, which implies
  that the diagonal entries of $A(y)$ are nonnegative, i.e., for all
  $i \in [n]$:
  \[
    \sum_{k = 1}^m (A^k)_{ii}\, y_k - (A^0)_{ii} \geq 0.
  \]
\item If for some $i$, $j \in [n]$, $A^0_{ij} \neq 0$, and $A^0_{ii} = 0$,
  $A^k_{ij} = 0$ for all $k \in [m]$, $A^k_{ii} = 0$ for all continuous
  variables and $\ell_i \geq 0$ for all integer variables,
  the following inequality is valid:
  \[
    \sum_{\substack{k \in \mathcal{I}:\\ A^k_{ii} > 0}} y_k \geq 1.
  \]
\item For all $X \succeq 0$, we have
  \[
    X_{ii} + X_{jj} - 2\, X_{ij} \geq 0,\qquad
    X_{ii} + X_{jj} + 2\, X_{ij} \geq 0.
  \]
  This follows by using a 2 by 2 minor and multiplying from left
  and right by the all-ones vector and $[1,-1]$, respectively. It also
  follows from~\eqref{eq:TwoByTwoMinorInequality}.
\item The previous inequalities translated to the matrix pencil are:
  \begin{align*}
    & \sum_{k=1}^m A^k_{ii}\, y_k - A^0_{ii} + \sum_{k=1}^m A^k_{jj}\, y_k - A^0_{jj} - 2 \Big(\sum_{k=1}^m A^k_{ij}\, y_k - A^0_{ij}\Big) \geq 0\\
    \quad\Leftrightarrow\quad&
    \sum_{k=1}^m \Big(A ^k_{ii} + A^k_{jj} - 2\, A^k_{ij}\Big)\, y_k \geq A^0_{ii} + A^0_{jj} - 2 A^0_{ij}.
  \end{align*}
\end{itemize}

All the above methods are implemented and help for some instances.


% -------------------------------------------------------------------------
\subsection{Presolving for $\boldsymbol{2 \times 2}$-minors}
\label{sec:2by2Minors}

\begin{lemma}
  Consider a solution $X$ to~\eqref{MISDP-P}. Then
  \[
    -\sqrt{U_{ii}\, U_{jj}} \leq X_{ij} \leq \sqrt{U_{ii}\, U_{jj}}
  \]
  holds for all $i$, $j \in [n]$.
\end{lemma}

\begin{proof}
  Since $X$ is positive semidefinite, we have
  \[
    X_{ii} X_{jj} - X_{ij}^2 \geq 0
    \quad\Leftrightarrow\quad
    X_{ij}^2 \leq X_{ii} X_{jj}.
  \]
  Moreover, since the diagonal terms of $X$ are nonnegative, we have
  $U_{ii} \geq L_{ii} \geq 0$ for all $i \in [n]$. Upper bounding via
  McCormick inequalities yields:
  \begin{align*}
    X_{ii} X_{jj} & \leq \min \{L_{jj}\, X_{ii} + U_{ii}\, X_{jj} - U_{ii}\, L_{jj},\;
                    U_{jj}\, X_{ii} + L_{ii}\, X_{jj} - L_{ii}\, U_{jj}\}  \\
    & \leq U_{ii}\, U_{jj}.
  \end{align*}
  Taking the square root gives the claim.
\end{proof}

This result can partly be translated to the matrix pencil format by defining
\[
  \tilde{U}_{ij} \define \sum_{k \in [m]: A^k_{ij} > 0} A^k_{ij}\, u_k -
  \sum_{k \in [m]: A^k_{ij} < 0} A^k_{ij}\, \ell_k - A^0_{ij}.
\]
Thus for any $y \in [\ell,u]$ we have $A(y)_{ij} \leq \tilde{U}_{ij}$. This
directly yields:

\begin{lemma}
  For any solution $y \in \R^m$ of~\eqref{MISDP}, we have
  \[
    -\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} \leq A(y)_{ij} \leq \sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}}
  \]
  for all $i$, $j \in [n]$.
\end{lemma}

Currently this method is implemented for the case in which each entry
$(i,j)$ is contained in exactly one matrix, i.e., there exists
$k = k(i,j) \in [m]$ such that
\[
  A(y)_{ij} = A^k_{ij}\, y_k - A^0_{ij}.
\]
In this case, we have
\[
  \frac{-\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} + A^0_{ij}}{A^k_{ij}} \leq y_k \leq
  \frac{\sqrt{\tilde{U}_{ii}\,\tilde{U}_{jj}} + A^0_{ij}}{A^k_{ij}}.
\]
Unfortunately, this has no effect on our test instances, i.e., the bounds
could never be strengthened across the large testset.

\begin{question}
  Can this be effective on some instance? Should this be applied
  nevertheless? Does it make sense to extend the method for the general
  case in~\eqref{MISDP}?
\end{question}

% -------------------------------------------------------------------------
\subsection{Implications of Trace Constraints}

The following is an extension of an observation in the context of computing
the RIP, see~\cite{GalP16}.

\begin{lemma}
  Consider~\eqref{MISDP-P} in which $\tr(X) \leq \alpha$ has to hold. Then
  \[
    -\tfrac{\alpha}{2} \leq X_{ij} \leq \tfrac{\alpha}{2}
  \]
  holds for all $i$, $j \in [n]$.
\end{lemma}

\begin{proof}
  Since $X$ is positive semidefinite, we have
  \[
    X_{ii} X_{jj} - X_{ij}^2 \geq 0
    \quad\Leftrightarrow\quad
    X_{ij}^2 \leq X_{ii} X_{jj}.
  \]
  We observe that $X_{ii} + X_{jj} \leq \alpha$ and $X_{ii}$,
  $X_{jj} \geq 0$. Thus
  \[
    X_{ii} X_{jj} \leq X_{ii} (\alpha - X_{ii}) = \alpha X_{ii} - X_{ii}^2.
  \]
  Taking the derivative and equating 0 yields a maximal point
  $X_{ii}^\star = \tfrac{\alpha}{2}$. Consequently,
  \[
    X_{ij}^2 \leq X_{ii} X_{jj} \leq \alpha X_{ii}^\star - (X_{ii}^\star)^2 =
    \tfrac{\alpha^2}{2} - \tfrac{\alpha^2}{4} = \tfrac{\alpha^2}{4}.
  \]
  Taking the square root shows the claim.
\end{proof}

\begin{question}
  Should this be transferred to~\eqref{MISDP} and how?
\end{question}

This is also implemented if each matrix entry arises from a single matrix
$A^k$ and $A^0$ is 0 for all involved entries. It does not find a
strengthening for the testset, but it strengthens the bounds for RIP
instances if the bounds have not been tightened before.

% -------------------------------------------------------------------------
\subsection{``Coefficient Tightening''}
\label{sec:CoefficientTightening}

For a linear constraint, coefficient tightening can be explained by the
following example: Consider two binary variables~$x$ and $y$ and the
constraint $x + 2\, y \geq 1$. Then the coefficient of $y$ can be tightened
to $1$ yielding $x + y \geq 1$.

Assume that all matrices $A^k$, $k \in [m]$, are psd and $y_k$ is binary
for $k \in [m]$. Then we can compute
\[
  \hat{\mu}_k = \min\, \{ \mu \geq 0 \suchthat \mu A^k - A^0 \succeq 0\}
\]
adn define $\mu_k \define \min\, \{\hat{\mu}_k, 1\}$. Note that $\mu_k = 1$ if
$\hat{\mu}_k = \infty$, i.e., if the above optimization problem is infeasible.

\begin{lemma}\label{lem:Tightening1}
  Let $k \in [m]$ with $A^k \succeq 0$. Then
  \[
    A^k - A^0 \succeq 0
    \quad\Leftrightarrow\quad
    \mu_k\, A^k - A^0 \succeq 0.
  \]
\end{lemma}

\begin{proof}
  Assume that $A^k - A^0 \succeq 0$. This implies that $\hat{\mu}_k \leq 1$
  and thus $\mu_k = \hat{\mu}_k$. This implies that
  $\mu_k\, A^k - A^0 \succeq 0$ by definition of $\hat{\mu}_k$.

  Conversely, assume that $\mu_k\, A^k - A^0 \succeq 0$. If $\mu_k = 1$, the
  statement is clear, so assume that $\mu_k = \hat{\mu}_k < 1$. Then for
  all $x \in \R^n$:
  \[
    0 \leq x\T \big(\mu_k\, A^k - A^0\big) x = \mu_k\,
    \underbrace{x\T A^k x}_{\geq 0} -
    x\T A^0 x \leq x\T (A^k - A^0) x,
  \]
  which implies that $A^k - A^0 \succeq 0$.
\end{proof}

\begin{lemma}\label{lem:TightenedProblem}
  Let $A^k \succeq 0$ for all $k \in [m]$ and $y \in \R^m$ with
  $y_i \in \{0,1\}$ for all integral variables $i \in I$ and $y_i \geq 0$
  for $i \notin I$. Then
  \[
    A(y) \succeq 0 \quad\Leftrightarrow\quad
    \sum_{k=1}^m \mu_k\, A^k\, y_k - A^0 \succeq 0,
  \]
  where we define $\mu_k = 1$ for $i \notin I$.
\end{lemma}

\begin{proof}
  First assume that
  $\sum_{k=1}^m \mu_k\, A^k\, y_k - A^0 \succeq 0$. Then for every
  $x \in \R^n$
  \[
    0 \leq x\T \bigg(\sum_{k=1}^m \mu_k\, A^k - A^0\bigg) x =
    \sum_{k=1}^m \mu_k\, \underbrace{x\T A^k x}_{\geq 0} -
    x\T A^0 x \leq x\T (\sum_{k=1}^m A^k - A^0) x,
  \]
  which implies that $A(y) \succeq 0$.

  We now assume that $A(y) \succeq 0$. By removing terms with $y_k = 0$ for
  $k \in I$, we can assume that $y_k = 1 $ for all $k \in I$. Thus,
  $\sum_{k=1}^m A^k - A^0 \succeq 0$. If $\mu_k = 1$ for all $k \in [m]$,
  then the statement is directly clear. Therefore assume that there exists
  $k \in I$ with $\mu_k = \hat{\mu}_k < 1$. But then already
  $\mu_k\, A^k - A^0 \succeq 0$. Adding the psd matrices $A^{\ell}$ for
  $\ell \in [m] \setminus \{k\}$ does not change this, which shows the
  claim.
\end{proof}

We now turn to the computation of $\mu_k$.

\begin{lemma}\label{lem:TightenPosDef}
  If $A^k \succ 0$ is positive definite, then
  \[
    \mu_k = \min \big\{ \lambda_{\max}(V^{-\top} A^0 V^{-1}),\, 1\big\}.
  \]
  where $A^k = V\T V$ for some invertible matrix $V \in \R^{n \times n}$.
\end{lemma}

\begin{proof}
  For some matrix $A \in \R^{n \times n}$ and invertible matrix
  $B \in \R^{n \times n}$ we have $A \succeq 0$ if and only if
  $B\T A B \succeq 0$ (see, e.g., Helmberg~\cite[Prop.~1.1.7]{Hel00}).
  Thus, $\mu\, A^k - A^0 \succeq 0$ if and only if
  \begin{align*}
    0 \preceq (V^{-1})^\top (\mu\, A^k - A^0) V^{-1} &= \mu\,
    (V^{-1})^\top A^k V^{-1} - (V^{-1})^\top A^0 V^{-1} \\
    &= \mu\, I - (V^{-1})^\top A^0 V^{-1}.
  \end{align*}
  The eigenvalues of $\mu\, I - B$ for some matrix $B$ are $\mu - \lambda$
  for eigenvalues $\lambda$ of $B$.\footnote{This follows, for example, by the
    general theorem that if $g(t) \in \R[t]$ is a scalar polynomial and
    $\lambda_1, \dots, \lambda_n$ are the eigenvalues of $B$, then the
    eigenvalues of $g(B)$ are $g(\lambda_1), \dots, g(\lambda_n)$, see
    Gantmacher~\cite[Ch.~IV, Thm.~3]{Gan59I}. We here use
    $g(t) = \mu - t = \mu\, t^0 - t^1$. (Alternatively, $(\mu I - B)v =
    \lambda v \Leftrightarrow Bv = (\mu - \lambda)v$, so $v$ is an
    eigenvector for eigenvalue $\mu - \lambda$.)}  thus, to make
  $\mu\, I - V^{-\top} A^0 V^{-1}$ psd, we need
  $\mu \geq \lambda_{\max}(V^{-\top} A^0 V^{-1})$.
\end{proof}

If $A^k$ is ``only'' positive semi-definite, one needs other methods that
we discuss now. Note that we want to find the largest $\mu$ such that
$\lambda_{\min}(\mu\, A^k - A^0) = 0$. It is therefore helpful to consider
$f(\mu) = \lambda_{\min}(\mu\, A^k - A^0)$.

The following is inspired by~\cite{Str16,HigSS16} and only stated here for completeness.

\begin{lemma}
  $f(\mu)$ is concave.
\end{lemma}

\begin{proof}
  We have for $\alpha \in [0,1]$ and $\mu_1$, $\mu_2 \geq 0$:
  \begin{align*}
    f\big(\alpha \mu_1 + (1 - \alpha) \mu_2\big)
    & = \lambda_{\min}\big((\alpha \mu_1 + (1 - \alpha) \mu_2) A^k - A^0\big)\\
    & = \lambda_{\min}\big(\alpha (\mu_1\, A^k - A^0) + (1 - \alpha) (\mu_2\, A^k - A^0)\big)\\
    & \geq \lambda_{\min}\big(\alpha (\mu_1\, A^k - A^0)\big) + \lambda_{\min}\big((1 - \alpha) (\mu_2\, A^k - A^0)\big)\\
    & = \alpha\, \lambda_{\min}\big(\mu_1\, A^k - A^0\big) + (1 - \alpha)\, \lambda_{\min}\big(\mu_2\, A^k - A^0\big)\\
    & = \alpha\, f(\mu_1) + (1 - \alpha)\, f(\mu_2),
  \end{align*}
  which shows the claim.
\end{proof}

\noindent
There are at least three choices to compute $\mu_k$:
\begin{itemize}[leftmargin=3ex]
\item One can use bisection within the interval $[0,1]$, checking whether
  $f(\mu) \geq 0$ in each step.
\item One can use Newton's method to compute $\mu_k$ similar
  to~\cite{Str16,HigSS16}.
\item One can solve the generalized eigenvalue problem $\mu A^k x = \lambda
  A^0 x$. It seems to be the case that most algorithms for this problem
  work best when one of the matrices is positive definite. But in this case
  we can apply Lemma~\ref{lem:TightenPosDef}. Otherwise, one can use the
  technique described in~\cite{Str16,HigSS16}. 
\end{itemize}

According to~\cite{Str16,HigSS16}, solving the generalized eigenvalue
problem is fastest, followed by bisection, and then Newton's method. But
note that the problems in~\cite{Str16,HigSS16} are slightly different
($f(\alpha) = \lambda_{\min}(\alpha A^k + (1 - \alpha) A^0)$).

The methods are implemented (using bisection) but none of the instances in
the testset allows a tightening.

\begin{question}
  Can these methods be successful and are they relevant?
\end{question}



% -------------------------------------------------------------------------
\subsection{Bound Tightening}

One can also use the methods in Section~\ref{sec:CoefficientTightening} to
compute bounds on each $y_k$ variable. For this we only need that all $A^k$
are psd, but $y_k$ does not need to be binary. We need the following:

\begin{lemma}
  Let $A^k \succeq 0$ for all $k \in [m]$ and $\ell \in [m]$. Then for
  $y \in \R^m$, $y \geq 0$:
  \[
    A(y) \succeq 0 \quad\Leftrightarrow\quad
    \alpha\, A^{\ell} + \sum_{\substack{k=1\\k \neq \ell}}^m A^k\, y_k - A^0 \succeq 0,
  \]
  where $\alpha = \min\,\{y_{\ell},\hat{\mu}_{\ell}\}$.
\end{lemma}

\begin{proof}
  Observe that if $\alpha = y_{\ell}$, then the statement is trivially
  true. Thus, let $\alpha = \hat{\mu}_k < y_{\ell}$.

  First assume that
  $\alpha\, A^{\ell} + \sum_{k=1,k \neq \ell}^m \hat{\mu}_k\, A^k\, y_k - A^0
  \succeq 0$.  Then for every $x \in \R^n$
  \begin{align*}
    0 & \leq x\T \bigg(\alpha\, A^{\ell} + \sum_{\substack{k=1\\k \neq \ell}}^m A^k\, y_k - A^0\bigg) x =
        \alpha\, \underbrace{x\T A^{\ell} x}_{\geq 0} + \sum_{\substack{k=1\\k \neq \ell}}^m x\T A^k
        x\, y_k - x\T A^0 x\\
      & \leq y_{\ell}\, x\T A^{\ell} x + \sum_{\substack{k=1\\k \neq \ell}}^m x\T A^k x\, y_k - x\T A^0 x
        = x\T A(y)x,
  \end{align*}
  which implies that $A(y) \succeq 0$.

  We now assume that $A(y) \succeq 0$. By definition of $\hat{\mu}_{\ell}$
  we have $\hat{\mu}_{\ell}\, A^{\ell} - A^0 \succeq 0$. Note that
  $A^k \, y_k$ is psd, since $A^k$ is psd and $y_k \geq 0$. Then adding
  these matrices for all for $k \in [m] \setminus \{\ell\}$ does not change
  the property of being psd, which shows the claim.
\end{proof}

From the lemma, we conclude that one can tighten the upper bounds of all
variables $y_{\ell}$ to $\min\, \{u_{\ell}, \hat{\mu}_{\ell}\}$. This is
implemented, but has not tightened any bound in presolving.



% -------------------------------------------------------------------------
\subsection{Other Presolving Methods}

We collect some more techniques here, all of which are currently not implemented.

\paragraph{Removing 0 rows/columns}

If for some $i \in [m]$ we have $A_{ij}^k = 0$ for all $j \in [n]$ and
$k \in [m]_0$, then row/column $i$ is not used and can be removed from the
matrices $A^k$.

\paragraph{Dual Fixing}

If for some $k \in [m]$ the matrix $A^k$ is psd and disjoint from the rest
(i.e., $A^k$ has no common nonzero with the other matrices), depending on
the objective coefficient one can fix $y_k$ to its upper bound.


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
\section{Useless or Already Implied Methods}


% -------------------------------------------------------------------------
\subsection{Implications of Primal Bounds}

The implications in the following results are automatically exploited for
linear constraints in problems of the form~\eqref{MISDP-P}. The argument
is standard argument, but we repeat it to see whether it can be exploited
somewhere.

\begin{lemma}
  Let $A \in \R^{n \times n}$, $\beta \in \R$ and define
  \[
    \mathcal{X} \define \{ X \in \R^{n\times n} \suchthat \skal{A}{X} \leq
    \beta,\; L_{ij} \leq X_{ij} \leq U_{ij}\; \forall (i,j) \in [n]^2\}.
  \]
  Then for any $X \in \mathcal{X}$ the inequality
  \[
    X_{st} \leq \frac{\beta - \alpha_{st}}{A_{st}}
  \]
  holds for all $(s,t) \in [n]^2$, where
  \begin{align*}
    & \alpha_{st} \define \sum_{(i,j) \in S^-_{st}} A_{ij} u_{ij} - \sum_{(i,j) \in S^+_{st}} A_{ij} \ell_{ij},\\
    & S^+_{st} = \{(i,j) \in [n]^2 \setminus \{(s,t)\} \suchthat A^k_{ij} > 0\},\\
    & S^-_{st} = \{(i,j) \in [n]^2 \setminus \{(s,t)\} \suchthat A^k_{ij} < 0\}.
  \end{align*}
\end{lemma}

\begin{proof}
  For any~$X \in \mathcal{X}$ we have
  \[
    \sum_{(i,j) \in [n]^2} A_{ij} X_{ij} \leq \beta
    \quad\Leftrightarrow\quad A_{st} X_{st} \leq \beta - \sum_{(i,j) \in [n]^2
      \setminus \{(s,t)\}} A_{ij} X_{ij}.
  \]
  Then
  \[
    A_{st} X_{st} \leq \beta - \sum_{(i,j) \in S^-_{st}} A_{ij} u_{ij} - \sum_{(i,j) \in S^+_{st}} A_{ij} \ell_{ij}.
  \]
  This yields the claim.
\end{proof}


%-------------------------------------------------------------------------
\subsection{Gershgorin-based Presolving}

Gershgorin's circle theorem says that each eigenvalue of a matrix
$A \in \R^{n \times n}$ lies in (at least) one of the Gershgorin discs
\[
  D_i \define \Big\{z \in \C \suchthat \abs{z - a_{ii}} \leq \sum_{j=1, j \neq
    i}^n \abs{a_{ij}} \Big\}.
\]
Thus, given a vector $y \in \R^m$, for $A(y) \succeq 0$ to hold, we need
that
\[
  \bigcup_{i=1}^n \bigg\{z \in \C \suchthat \abs{z - A(y)_{ii}} \leq \sum_{j=1, j \neq
    i}^n \abs{A(y)_{ij}}\bigg\} \cap \{z \in \C \suchthat \Re(z) \geq 0\}
  \neq \varnothing.
\]
But since we need to have $A(y)_{ii} \geq 0$ for all $y$ with
$A(y) \succeq 0$, the above condition is always fulfilled: The center
points of the circles are always in the nonnegative complex halfplane.


% -------------------------------------------------------------------------


% -------------------------------------------------------------------------
\subsection{SOCP-inspired Presolving}

Consider a second order cone constraint $(x,t) \in L_{n+1}$, where
\[
  L_{n+1} \define \Big\{x \in \R^{n+1} \suchthat \sqrt{x_1^2 + \dots + x_n^2}
  \leq x_{n+1}\Big\}
\]
is the Lorentz cone.  Presolving/propagation of
$\sqrt{x_1^2 + \dots + x_n^2} \leq t$ with bounds $\ell \leq x \leq u$
yields for some $k \in [n]$:
\begin{equation}\label{eq:SOCPpre}
  x_k^2 \leq t^2 - \sum_{\substack{i=1\\ i \neq k}}^n x_i^2 \leq t^2 -
  \sum_{\substack{i=1\\ i \neq k}} \gamma_i \leq t^2,\qquad
  \gamma_i \define \min_{\gamma \in [\ell_i, u_i]} \gamma^2.
\end{equation}
Note that $\ell_i < 0 < u_i$ is possible. One main application is the case
in which there is an upper bound on~$t$. We would like to have a similar
presolving (propagation) for SDPs -- at least if we would write an SOCP as
an SDP, but possibly for more general cases.

Indeed, SOCP is special case of SDP, because:
\[
  (x,t) \in L_{n+1} \quad\Leftrightarrow\quad
  \tilde{X} \define
  \begin{pmatrix}
    t & x\T\\
    x & I_n
  \end{pmatrix}
  \succeq 0.
\]
To see this, we need the following result adapted
from~\cite[Section~A.5.5]{BoyV09}. Let
\[
  D = \begin{pmatrix}
    A & B\T\\
    B & C
  \end{pmatrix}
\]
with a positive definite $C \succ 0$. Then $D \succeq 0$ if and only if
$S = A - B\T C^{-1} B \succeq 0$ and $A \succeq 0$\footnote{Note that the
  condition $A \succeq 0$ is missing in~\cite{BoyV09}.}. For $D = \tilde{X}$, we
get: $\tilde{X} \succeq 0$ if and only if $t \geq 0$, $t - x\T x \geq
0$. The latter is equivalent to $(x,t) \in L_{n+!}$.

Now consider a more general matrix
\begin{equation}\label{eq:GeneralPSD}
  \begin{pmatrix}
    t & x\T\\
    x & X
  \end{pmatrix}
  \succeq 0
\end{equation}
with an upper bound $\bar{t}$ on $t$. This seems to be the most natural
generalization of the SOCP case.

Taking $2 \times 2$ minors for some $i \in [n]$, we get
$t X_{ii} - x_i^2 \geq 0$. Thus, $x_i^2 \leq \bar{t} X_{ii}$, which leads
to
\[
  - \sqrt{\bar{t} X_{ii}} \leq x_i \leq \sqrt{\bar{t} X_{ii}}.
\]
Note that this is covered by the bounds considered in
Section~\ref{sec:2by2Minors}. In the SOCP case, we obtain
$x_i \leq \sqrt{\bar{t}}$, which only provides the weakest upper bound
in~\eqref{eq:SOCPpre}.

Thus, one question is whether one can derive strengthened bounds. This is
currently unclear. As one idea, one can consider generalized Schur
polynomials (since $X$ is not necessarily positive definite, one needs the
generalization), which we adapt from~\cite[Section~A.5.5]{BoyV09}: Consider
again the matrix $D$ from above. Then
\[
  D \succeq 0 \quad\Leftrightarrow\quad
  A \succeq 0,\; (I - AA^{+})B\T = 0,\; C - B A^{+} B\T \succeq 0,
\]
where $A^+$ is the pseudo-inverse of $A$. Reversing the roles of $A$ and
$C$ we get:
\[
  D \succeq 0 \quad\Leftrightarrow\quad
  C \succeq 0,\; (I - CC^{+})B = 0,\; A - B\T C^{+} B \succeq 0.
\]
Applied to~\eqref{eq:GeneralPSD}, we obtain from the first:
\begin{equation}\label{eq:GenSchur1}
  t \geq 0,\; (1 - t t^{-1}) x\T = 0,\; X - x t^{-1} x\T \succeq 0,\; 
\end{equation}
if $t > 0$ and from the second:
\begin{equation}\label{eq:GenSchur2}
  X \succeq 0,\; (I - XX^{+})x = 0,\; t - x\T X^{+} x \succeq 0.
\end{equation}

Using
$X - \tfrac{1}{t} x x\T \succeq 0 \Leftrightarrow t X - x x\T \succeq 0$
from~\eqref{eq:GenSchur1}, we get for the $2 \times 2$-minors with $i$,
$j \in [n]$, $i \neq j$:
\begin{align*}
  & (t\, X_{ii} - x_i^2) (t\, X_{jj} - x_j^2) -  (t\, X_{ij} - x_i\, x_j)^2 \geq 0
  \\
  \Leftrightarrow\quad & t \, X_{ij} - x_i\, x_j \leq \sqrt{t\, X_{ii} - x_i^2} \sqrt{t\, X_{jj} - x_j^2}
  \\
  \Leftrightarrow\quad & X_{ij} \leq \frac{1}{t} \bigg(\sqrt{t\, X_{ii} - x_i^2}
                         \sqrt{t\, X_{jj} - x_j^2} + x_i\, x_j\bigg).
\end{align*}
This, for instance, yields
\[
  X_{ij} \leq \sqrt{U_{ii}} \sqrt{U_{jj}} + \tfrac{1}{t} x_i\, x_j.
\]
which does not seem to help. If we would know that $X_{ij} = x_i\, x_j$,
i.e., $X$ is rank-1, then we obtain
\[
  X_{ij} \leq \frac{\sqrt{t\, X_{ii} - x_i^2} \sqrt{t\, X_{jj} - x_j^2}}{t-1}.
\]


\begin{question}
  Can any of this be made useful?
\end{question}


% -------------------------------------------------------------------------
\section{Specialized Arguments for Computing the RIP}

\subsection{Sparse Principal Component Analysis}

Consider the sparse principal component analysis (PCA) problem
\[
  \max\; \{\Norm{Ax}{2}^2 \suchthat \Norm{x}{2}^2 = 1,\; \Norm{x}{0} \leq k\}. 
\]
with $A \in \R^{m \times n}$.  Let $x^*$ be an optimal solution and
$S = \supp(x^*)$ be its support with $k \define \Norm{x}{0}$. Consider the
submatrix~$A_S \in \R^{m \times k}$ indexed by columns in $S$. Then
$\tilde{A} = A_S\T A_S^{\phantom{T}} \in \R^{k \times k}$ is symmetric positive
semidefinite. By the Rayleigh-Ritz theorem (see
Helmberg~\cite[Thm.~A.0.4]{Hel00}) we have
\[
  \max_{y \in \R^k} \; \{\Norm{\tilde{A}y}{2}^2 \suchthat \Norm{y}{2}^2 =
  1\} = \lambda_{\max}(\tilde{A}),
\]
i.e., the sparse PCA problem looks for a $k$-sparse vector yielding the
largest eigenvalue of the supporting submatrix. A similar statement holds
for minimization variant and the minimal eigenvalue.

A formulation for the sparse PCA problem is
\begin{subequations}\label{eq:SPCA}
  \begin{align}
    \max \quad & \skal{A\T A}{X} \\
    \text{s.t.} \quad & \tr(X) = 1, \\
    & -z_j \leq X_{ij} \leq z_j \quad \text{for } i,\; j \in [n],\label{eq:SPCAcouple} \\
    & \sum_{i=1}^n z_i \leq k,\\
    & X \succeq 0, \\
    & z \in \{0,1\}^n.
  \end{align}
\end{subequations}
In~\cite{GalP16} (and~\cite{LiX20}) it is proved that there exists an
optimal rank-1 solution~$X^*$. Thus, $X^* = x^* (x^*)\T$ for some
$x^* \in \R^n$ with $\Norm{x^*}{0} \leq k$. Let $S = \supp(x^*)$. Then
$x^*_S$ is an eigenvector for a maximal eigenvalue of
$A_S\T A_S^{\phantom{T}}$.

\begin{question}
  Since $X_{ii} = 0$ implies $X_{ij} = 0$ for all $j \in [n]$, it would
  suffice to only consider the inequalities $-z_i \leq X_{ii} \leq x_i$ for
  all $i \in [n]$ in~\eqref{eq:SPCAcouple}. Can we detect that the other
  inequalities are implied and remove them? Does this speed up the solution
  process?
\end{question}

We apply the Perron-Frobenius theorem. To this end, we define a matrix $A$ to
be reducible if there exists a permutation matrix $P$ such that
\[
  P\T A P = \begin{pmatrix} B & C \\ 0 & D \end{pmatrix}.
\]
Otherwise it is \emph{irreducble}. We have the following:

\begin{theorem}[{Frobenius Theorem, see Gantmacher~\cite[Chapter 2, Thm.~2]{Gan59II}}]\label{thm:PerronFrobenius}
  An irreducible nonnegative matrix $A$ has a positive maximal eigenvalue
  $\lambda > 0$. The corresponding eigenvectors have positive entries.
\end{theorem}

For general (i.e., also reducible) matrices the following holds:
\begin{theorem}[{Gantmacher~\cite[Chapter 2, Thm.~3]{Gan59II}}]\label{thm:PerronFrobeniusNonnegative}
  A nonnegative matrix $A$ has a nonnegative maximal eigenvalue
  $\lambda \geq 0$. The corresponding eigenvectors have nonnegative
  entries.
\end{theorem}

Moreover, Gantmacher proves the following.
\begin{lemma}[{\cite[Chapter 2, Remark 3]{Gan59II}}]\label{lemma:PFNegative}
  An irreducible nonnegative matrix $A$ cannot have two linearly
  independent nonnegative eigenvectors.
\end{lemma}
This implies that all eigenvectors to all other eigenvalues cannot be nonnegative!
For oscillaritory matrices some more can be said about the signs, see
\cite[Chapter 2, Theorem~13]{Gan59II}.

\begin{lemma}
  If $A_S \geq 0$, then $x^* \geq 0$.
\end{lemma}

\begin{proof}
  Since $A_S \geq 0$, Theorem~\ref{thm:PerronFrobeniusNonnegative} implies
  that $A_S$ has a maximal eigenvalue with corresponding \emph{nonnegative}
  eigenvector. This eigenvector is unique up to scaling.
\end{proof}

Thus, one can restrict $X \geq 0$ and write
\[
  0 \leq X_{ij} \leq z_j \quad \text{for } i,\; j \in [n]
\]
instead of~\eqref{eq:SPCAcouple}.

\paragraph{A Valid SOCP Constraint}

Li and Xie~\cite{LiX20} show that the following inequalitiy is valid for
sparse PCA:
\[
  \sum_{j \in [n]} X_{ij}^2 \leq X_{ii}\, z_i \quad\text{for all } i \in [n].
\]
This holds, since in this case $X = x x\T$ and $\sum_{j=1}^m x_j^2 =
1$. Thus,
\[
  \sum_{j \in [n]} X_{ij}^2 \leq \sum_{j \in [n]} x_i^2\, x_j^2 = x_i^2
  \leq z_i\, X_{ii}.
\]
Note that these inequalities can be reformulated by subtracting $X_{ii}^2$
from both sides to yield
\[
  \sum_{\substack{j \in [n]\\ j \neq i}} X_{ij}^2 \leq z_i\, (X_{ii} - X_{ii}^2).
\]




These inequalities are SOCP representable as
\begin{align*}
  & (X_{i1}, \dots, X_{in}, \tfrac{X_{ii} - z_i}{2}, \tfrac{X_{ii} + z_i}{2}) \in L_{n+2},\\
  \Leftrightarrow\quad & \sum_{j=1}^n X_{ij}^2 + \tfrac{1}{4}(X_{ii}^2 - 2\,
                    z_i\, X_{ii} + z_i^2) \leq \tfrac{1}{4}(X_{ii}^2 + 2\, z_i\, X_{ii} + z_i^2)\\
  \Leftrightarrow\quad & \sum_{j=1}^n X_{ij}^2 \leq z_i\, X_{ii}.
\end{align*}
Formulated as an SDP, we obtain
\[
  \begin{pmatrix}
    \tfrac{X_{ii} + z_i}{2} & X_{i1} & \dots & X_{in} & \tfrac{X_{ii} - z_i}{2}\\
    X_{i1} & 1 &  & 0 & 0\\
    \vdots & & \ddots & & \vdots\\
    X_{in} & &    & 1 & 0\\
    \tfrac{X_{ii} - z_i}{2} & & \dots & 0 & 1
  \end{pmatrix}
  \succeq 0,
\]
for all $i \in [n]$.  These SDP blocks can in principle be added
to~\eqref{eq:SPCA}. Alternatively, one can encode the weaker inequality
$X_{ij}^2 \leq z_i\, X_{ii}$ as
\[
  \begin{pmatrix}
    z_i & X_{ij}\\
    X_{ij} & X_{ii}
  \end{pmatrix}
  \succeq 0,
\]
but this would require quadratically many SDP-constraints.

Moreover, since $X_{ii} \leq x_i$, we have the weaker
$\sum_{j=1}^n X_{ij}^2 \leq z_i^2$, which is equivalent to
\[
  \begin{pmatrix}
    z_i & X_{i1} & \dots & X_{in} \\
    X_{i1} & 1 &  & 0 \\
    \vdots & & \ddots & \vdots \\
    X_{in} & &    & 1 \\
  \end{pmatrix}
  \succeq 0.
\]

\begin{question}
  Can these inequalities be useful within SCIP-SDP?
\end{question}

\begin{question}
  Is there any inequality that exploits the fact that $z_i^2 = z_i$ for
  feasible solutions?
\end{question}

\subsection{Minimization Variant}

Now consider the lower problem
\[
  \min_{y \in \R^k} \; \{\Norm{\tilde{A}y}{2}^2 \suchthat \Norm{y}{2}^2 =
  1\} = \lambda_{\min}(\tilde{A}).
\]
Unfortunately, by Lemma~\ref{lemma:PFNegative}, a vector for an eigenvalue
that is not equal to the (unique) maximal eigenvalue always has positive
and negative entries.

Nevertheless if the matrix $A\T A$ is \emph{positive definite}, we can
solve the maximization version in~\eqref{eq:SPCA} where $A\T A$ is replaced
by $(A\T A)^{-1}$. This holds since $\lambda$ is an eigenvalue of $A$ if
and only if $1/\lambda$ is an eigenvalue of $A^{-1}$. This transformation
might have the advantage that the maximization version is sometimes easier
to solve, but this might only hold for nonnegative matrices $A$. Note,
however, that $(A\T A)^{-1}$ is very unlikely to be nonnegative, so we
cannot benefit from the application of the Perron-Frobenius theorem here.

\begin{question}
  Can the transformation help to speed up the solution process? How often
  are the matrices positive definite?
\end{question}


\begin{small}
  \bibliographystyle{abbrv}
  \bibliography{MISDPpresolving.bib}
\end{small}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
